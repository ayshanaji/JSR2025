---
title: "Flexible Prior-Gene50- Hierarchical"
author: "Ayisha - TRU- T00727585"
date: "2024-11-12"
output: html_document
---


```{r}
# Load necessary libraries
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
library(ISLR)
library(dplyr)
library(tidyr)
library(coda)
library(reshape)
library(stats4)
library(MCMCpack)
library(ggplot2)
library(MASS)
library(Matrix)
library(knitr)
library(readr)
library(mvtnorm)
library(stargazer)
library(pROC)
library(caret)
library(gridExtra)
library(grid)
```


```{r}
# Load data
trainData <- read.csv("training_data_tt.csv", header = TRUE, stringsAsFactors = FALSE)
#validationData <- read.csv("validation_data.csv", header = TRUE, stringsAsFactors = FALSE)
testData <- read.csv("test_data_tt.csv", header = TRUE, stringsAsFactors = FALSE)

# Define logistic log-likelihood function
flogis.log <- function(beta, zlv, X, y) {
  p <- plogis(X %*% (beta * zlv))
  sum(ifelse(y, log(p), log(1 - p)))
}

# Prepare data for analysis, excluding the intercept
y_train <- trainData[, "DSS"]
X_train <- model.matrix(as.formula(paste("DSS ~ .")), data = trainData)  
#y_valid <- validationData[, "DSS"]
#X_valid <- model.matrix(as.formula(paste("DSS ~ .")), data = validationData)
y_test <- testData[, "DSS"]
X_test <- model.matrix(as.formula(paste("DSS ~ . ")), data = testData)

# Update k to reflect the new number of columns without the intercept
k <- ncol(X_train)
n <- nrow(X_train)


```

```{r}
# Step 1: Performing Frequentist Logistic Regression for Initial Beta Estimates
reg.glm <- glm(DSS ~ .  , data = trainData, family = binomial)
BETA_init <- as.vector(reg.glm$coef)
vcov_matrix <- vcov(reg.glm)


# Step 2: Calculating the Prior Mean and Standard Deviation for Beta
pmn.beta <- BETA_init
psd.beta <- vcov_matrix

```


```{r}
# Initial setup for MCMC
S <- 140000  # Number of MCMC iterations
c <- 1       # Proposal variance tuning parameter
k <- ncol(X_train)  # Number of predictors (confirm this matches)
beta <- BETA_init
alpha1 <- 1
alpha2 <- 1
# Initialize zlv based on global theta
global_theta <- rbeta(1, alpha1, alpha2)  # Initial global theta from Beta prior
theta_j <- rbeta(k, global_theta, 1 - global_theta)  # Initialize each theta_j from global theta

zlv <- rbinom(k, 1, theta_j)  # Initial inclusion/exclusion indicators for predictors

# Initialize matrices and counters
acsb <- acsg <- 0
BETA <- matrix(0, nrow = S, ncol = k)   # Store samples of beta for each iteration
ZLV <- matrix(0, nrow = S, ncol = k)    # Store samples of zlv for each iteration
THETA <- matrix(0, nrow = S, ncol = k)       # Store samples for each theta_j
accept_beta <- rep(0, k)                # Acceptance count for beta updates
accept_zlv <- rep(0, k)                 # Acceptance count for zlv updates
global_theta_values <- numeric(S)                  # Store samples for global theta
short_chain_length<-1000
# Confirm dimensions
print(dim(X_train))  # Should match (n, k)
print(length(beta))  # Should match k
print(length(zlv))   # Should match k
print(dim(vcov_matrix)) 
# Proposal Variance for MCMC
var.prop <- c * vcov_matrix  


```



```{r}
# Track acceptance rate
set.seed(1)  # For reproducibility
start_time <- Sys.time()
for (i in 1:10) {  # Adjust the proposal variance over 10 iterations of tuning
  acs <- 0  # Acceptance count
  
  for (s in 1:short_chain_length) {
    # Propose a new beta using current variance scale (c * vcov_matrix)
    beta.p <- t(rmvnorm(1, beta, c * vcov_matrix))
    
    # Calculate log acceptance ratio (log-hastings ratio)
    lhr <- flogis.log(beta.p, zlv, X_train, y_train) - flogis.log(beta, zlv, X_train, y_train) +
           dmvnorm(t(beta.p), BETA_init, vcov_matrix, log = TRUE) -
           dmvnorm(t(beta), BETA_init, vcov_matrix, log = TRUE)
    
    # Accept or reject based on acceptance ratio
    if (log(runif(1)) < lhr) {
      beta <- beta.p
      acs <- acs + 1
    }
  }
  
  # Calculate acceptance rate
  acceptance_rate <- acs / short_chain_length
  print(paste("Iteration", i, "- Acceptance rate:", acceptance_rate))
  
  # Adjust `c` based on acceptance rate
  if (acceptance_rate < 0.2) {
    c <- c / 2  # Reduce `c` if acceptance rate is too low
  } else if (acceptance_rate > 0.5) {
    c <- c * 2  # Increase `c` if acceptance rate is too high
  }
  
  # Print adjusted `c` value for tracking
  print(paste("Adjusted proposal variance scaling factor (c):", c))
}

# After tuning, `c` can be used in the main MCMC chain
print(paste("Final proposal variance scaling factor (c):", c))
```


```{r}
# Initial setup for MCMC
S <- 140000  # Number of MCMC iterations      
k <- ncol(X_train)  # Number of predictors (confirm this matches)
beta <- BETA_init  # Initial beta coefficients from frequentist logistic regression
alpha1 <- 1
alpha2 <- 1

# Initialize global theta and individual theta_j
global_theta <- rbeta(1, alpha1, alpha2)  # Global sparsity parameter
theta_j <- rbeta(k, global_theta, 1 - global_theta)  # Individual inclusion probabilities

# Initialize zlv (latent variable for predictor inclusion/exclusion)
zlv <- rbinom(k, 1, theta_j)  # Initial inclusion indicators

# Initialize matrices and counters for MCMC sampling
BETA <- matrix(0, nrow = S, ncol = k)   # Store samples of beta
ZLV <- matrix(0, nrow = S, ncol = k)    # Store samples of zlv
THETA <- matrix(0, nrow = S, ncol = k)  # Store samples of theta_j
GLOBAL_THETA <- numeric(S)              # Store samples of global theta
acsb <- acsg <- accept_global_theta <- 0  # Acceptance counters

# Proposal variance for beta updates
var.prop <- c * vcov_matrix  # Covariance matrix from frequentist regression

# MCMC loop
for (s in 1:S) {
  
  # Step 1: Metropolis-Hastings update for beta
  beta.p <- t(rmvnorm(1, beta, var.prop))  # Propose new beta
  lhr <- flogis.log(beta.p, zlv, X_train, y_train) - flogis.log(beta, zlv, X_train, y_train) +
         dmvnorm(t(beta.p), BETA_init, vcov_matrix, log = TRUE) -
         dmvnorm(t(beta), BETA_init, vcov_matrix, log = TRUE)
  
  if (log(runif(1)) < lhr) {
    beta <- beta.p
    acsb <- acsb + 1
  }
  BETA[s, ] <- beta
  
  # Step 2: Metropolis-Hastings update for each component of zlv
  for (j in 1:k) {
    zlv.p <- zlv
    zlv.p[j] <- 1 - zlv.p[j]  # Flip the j-th component
    
    lhg <- flogis.log(beta, zlv.p, X_train, y_train) - flogis.log(beta, zlv, X_train, y_train) +
           sum(dbinom(zlv.p[j], size = 1, prob = theta_j[j], log = TRUE)) - 
           sum(dbinom(zlv[j], size = 1, prob = theta_j[j], log = TRUE))
    
    if (log(runif(1)) < lhg) {
      zlv[j] <- zlv.p[j]
      acsg <- acsg + 1
    }
  }
  ZLV[s, ] <- zlv
  
  # Step 3: Gibbs update for each theta_j from Beta(global_theta, 1 - global_theta)
  for (j in 1:k) {
    theta_j[j] <- rbeta(1, global_theta + zlv[j], (1 - global_theta) + (1 - zlv[j]))
  }
  THETA[s, ] <- theta_j
  
  # Step 4: Metropolis-Hastings update for global theta
  proposed_global_theta <- rbeta(1, alpha1 + sum(zlv), alpha2 + k - sum(zlv))  # Propose new global theta
  
  # Calculate log acceptance ratio for global theta
  theta_safe <- pmax(pmin(theta_j, 0.999), 0.001)  # Avoid issues with 0 or 1
  current_likelihood <- sum(dbeta(theta_safe, global_theta, 1 - global_theta, log = TRUE))
  proposed_likelihood <- sum(dbeta(theta_safe, proposed_global_theta, 1 - proposed_global_theta, log = TRUE))
  lhr_theta <- proposed_likelihood - current_likelihood + 
               dbeta(proposed_global_theta, alpha1, alpha2, log = TRUE) -
               dbeta(global_theta, alpha1, alpha2, log = TRUE)
  
  if (log(runif(1)) < lhr_theta) {
    global_theta <- proposed_global_theta
    accept_global_theta <- accept_global_theta + 1
  }
    GLOBAL_THETA[s] <- global_theta
  
  # Debug output every 1000 iterations
  if (s %% 1000 == 0) {
    cat("Iteration:", s, "- Global theta:", global_theta, "- beta[1]:", beta[1], "- zlv:", zlv, "\n")
  }
}

# Display acceptance rates after MCMC loop
cat("Acceptance rate for beta updates:", acsb / S, "\n")
cat("Acceptance rate for zlv updates:", acsg / (S * k), "\n")
cat("Acceptance rate for global theta updates:", accept_global_theta / S, "\n")
end_time <- Sys.time()
print(paste("Total computing time OAM1:", end_time - start_time))
units(end_time - start_time)

```



```{r,eval=FALSE}
# -------------------------------------------
# Assuming you have already completed MCMC and stored:
# GLOBAL_THETA: Vector of global theta samples (S iterations)
# THETA: Matrix of theta_j samples (S rows = iterations, k columns = predictors)
# -------------------------------------------

# -------------------------------------------
# PARAMETERS for Plotting
# -------------------------------------------
S <- length(GLOBAL_THETA)  # Number of MCMC iterations
k <- ncol(THETA)  # Number of predictors

# -------------------------------------------
# STEP 1: Thinning (to reduce points plotted)
# -------------------------------------------
thin_every <- 500  # Keep every 500th sample
thin_index <- seq(1, S, by = thin_every)

# Thinned versions of global theta and theta_j
GLOBAL_THETA_thin <- GLOBAL_THETA[thin_index]
THETA_thin <- THETA[thin_index, ]

# -------------------------------------------
# STEP 2: Select a few θ_j to plot (e.g., 4 predictors)
# -------------------------------------------
set.seed(123)  # For reproducibility
selected_j <- sample(1:k, 4)  # Randomly select 4 predictors to visualize

# -------------------------------------------
# STEP 3: Optional Smoothing function (Moving Average)
# -------------------------------------------
smooth_line <- function(x, window = 100) {
  stats::filter(x, rep(1 / window, window), sides = 2)
}

# Smooth global theta
GLOBAL_THETA_smooth <- smooth_line(GLOBAL_THETA_thin)

# Smooth selected theta_j
THETA_smooth <- apply(THETA_thin[, selected_j, drop = FALSE], 2, smooth_line)

# -------------------------------------------
# STEP 4: Plotting
# -------------------------------------------
# Set colors and line types
colors <- c("red", "blue", "purple", "darkgreen")
linetypes <- c(2, 3, 4, 5)

# Plot Global Theta first
plot(GLOBAL_THETA_smooth, type = 'l', col = 'orange', lwd = 3,
     ylim = c(0, 1), xlab = "Iteration", ylab = "Value",
     main = expression("Evolution of " * theta * " and " * theta[j] * " during MCMC"))

# Add smoothed theta_j lines
for (i in 1:length(selected_j)) {
  lines(THETA_smooth[, i], col = colors[i], lty = linetypes[i], lwd = 2)
}

# -------------------------------------------
# STEP 5: Add Legend
# -------------------------------------------
legend("topright",
       legend = c("Global θ", paste0("θ_", selected_j)),
       col = c("orange", colors),
       lty = c(1, linetypes),
       lwd = 3)

# -------------------------------------------
# OPTIONAL: Save the figure
# -------------------------------------------

# Uncomment the following block to save as PNG:
# png("theta_evolution_fixed.png", width = 1000, height = 700, res = 150)
# (Repeat plotting code here)
# dev.off()

# -------------------------------------------
# END OF CODE
# -------------------------------------------

```


```{r,eval=FALSE}
# Smoothing function with safe center
smooth_line_safe <- function(x, window = 200) stats::filter(x, rep(1 / window, window), sides = 2)

# Apply smoothing
GLOBAL_THETA_smooth <- smooth_line_safe(GLOBAL_THETA_thin)
THETA_smooth <- apply(THETA_thin[, selected_j, drop = FALSE], 2, smooth_line_safe)

# Find valid indices (non-NA)
valid_indices <- which(!is.na(GLOBAL_THETA_smooth))

# Subset valid data
GLOBAL_THETA_smooth <- GLOBAL_THETA_smooth[valid_indices]
THETA_smooth <- THETA_smooth[valid_indices, ]

# Plot
plot(GLOBAL_THETA_smooth, type = 'l', col = 'orange', lwd = 3,
     ylim = c(0, 1), xlab = "Iteration", ylab = "Value",
     main = expression("Evolution of " * theta * " and " * theta[j] * " during MCMC"))

# Add selected theta_j lines
colors <- c("red", "blue", "purple", "darkgreen")
linetypes <- c(2, 3, 4, 5)
for (i in 1:length(selected_j)) {
  lines(THETA_smooth[, i], col = colors[i], lty = linetypes[i], lwd = 2)
}

# Add legend
legend("topright", legend = c("Global θ", paste0("θ_", selected_j)),
       col = c("orange", colors), lty = c(1, linetypes), lwd = 3)

```


```{r,eval=FALSE}
# ---------------------
# Assumes you have:
# GLOBAL_THETA: vector of global theta samples (S iterations)
# THETA: matrix of theta_j samples (S rows = iterations, k columns = predictors)
# ---------------------

# ---------------------
# Parameters and Setup
# ---------------------
S <- length(GLOBAL_THETA)  # Number of MCMC iterations
k <- ncol(THETA)  # Number of predictors
thin_every <- 100  # Thinning parameter
window_size <- 200  # Smoothing window

# ---------------------
# STEP 1: Thinning
# ---------------------
thin_index <- seq(1, S, by = thin_every)  # Keep every nth sample

GLOBAL_THETA_thin <- GLOBAL_THETA[thin_index]
THETA_thin <- THETA[thin_index, ]

# ---------------------
# STEP 2: Smoothing Function (Safe Moving Average)
# ---------------------
smooth_line_safe <- function(x, window = window_size) stats::filter(x, rep(1 / window, window), sides = 2)

# ---------------------
# STEP 3: Apply Smoothing
# ---------------------
GLOBAL_THETA_smooth <- smooth_line_safe(GLOBAL_THETA_thin)
THETA_smooth <- apply(THETA_thin, 2, smooth_line_safe)

# ---------------------
# STEP 4: Remove NA caused by smoothing
# ---------------------
valid_indices <- which(!is.na(GLOBAL_THETA_smooth))  # Non-NA indices

GLOBAL_THETA_smooth <- GLOBAL_THETA_smooth[valid_indices]
THETA_smooth <- THETA_smooth[valid_indices, ]

# ---------------------
# STEP 5: Select subset of theta_j to visualize (optional)
# ---------------------
set.seed(123)  # For reproducibility
num_variables_to_plot <- 50  # Adjust number of theta_j to plot
selected_j <- sample(1:k, num_variables_to_plot)  # Randomly select predictors

# ---------------------
# STEP 6: Plotting
# ---------------------
# Plot global theta first
plot(GLOBAL_THETA_smooth, type = 'l', col = 'orange', lwd = 3,
     ylim = c(0, 1), xlab = "Iteration", ylab = "Inclusion Probability",
     main = expression("Evolution of Global " * theta * " and " * theta[j] * " During MCMC"))

# Add theta_j lines
colors <- rainbow(num_variables_to_plot)  # Auto-generate colors for each variable

for (i in 1:num_variables_to_plot) {
  lines(THETA_smooth[, selected_j[i]], col = colors[i], lty = 2, lwd = 2)
}

# ---------------------
# STEP 7: Legend
# ---------------------
legend_labels <- c("Global θ", paste0("θ_", selected_j))
legend("right", legend = legend_labels,
       col = c("orange", colors), lty = c(1, rep(2, num_variables_to_plot)), lwd = 3)

# ---------------------
# OPTIONAL: Save as High-Resolution Image
# ---------------------
# png("theta_evolution_all.png", width = 1200, height = 800, res = 200)
# (Repeat plotting code here)
# dev.off()

# ---------------------
# END OF CODE
# ---------------------

```

```{r}
# Debugging outputs
print(global_theta)
print(proposed_global_theta)
print(current_likelihood)
print(proposed_likelihood)

```


```{r}
# Save the results
save(BETA, ZLV,THETA,GLOBAL_THETA,theta_j, file = "OAM2_results_FIMS.RData")

# To load the results back in future sessions
load("OAM2_results_FIMS.RData")
```

```{r}
# Load necessary libraries
library(coda)
library(gridExtra)
library(ggplot2)

variable_names <- colnames(X_train)
print(paste("Variable Names:", variable_names))

# Ensure we have names for each column in BETA and ZLV
colnames(BETA) <- variable_names
colnames(ZLV) <- variable_names
BETA_ZLV<-BETA *ZLV
colnames(BETA_ZLV) <- variable_names
# 1. Effective Sample Size (ESS) for `BETA` and `ZLV`
ess_beta <- effectiveSize(as.mcmc(BETA))
ess_zlv <- effectiveSize(as.mcmc(ZLV))
ess_beta_zlv <- effectiveSize(as.mcmc(ZLV))

# Display ESS for both in a summary table
ess_summary <- data.frame(
  Parameter = variable_names,
  ESS_Beta = round(ess_beta, 2),
  ESS_ZLV = round(ess_zlv, 2),
  ESS_BETA_ZLV= round(ess_beta_zlv, 2)
)

# Save ESS Summary as a PDF
pdf("ESS_Summary_Beta_ZLV_DSSM2.pdf", width = 8, height = 5)
grid.table(ess_summary)
dev.off()

# 2. Trace and Density Plots for Both `BETA` and `ZLV`

# Generate Trace and Density Plots for `BETA`
pdf("Beta_Trace_Density_Plots_DSSM2.pdf", width = 14, height = 10)  # Adjust dimensions
par(mfrow = c(3, 4), mar = c(4, 4, 2, 1))  # 4x4 grid
for (i in 1:ncol(BETA)) {
  plot(BETA[, i], type = "l", main = paste("Trace Plot for Beta", variable_names[i]), 
       xlab = "Iteration", ylab = variable_names[i])
  plot(density(BETA[, i]), main = paste("Density for Beta", variable_names[i]), 
       xlab = variable_names[i], ylab = "Density")
}
dev.off()

# Generate Trace and Density Plots for `BETA_ZLV`
pdf("BETA_ZLV_Trace_Density_Plots_DSSM2.pdf", width = 14, height = 10)
par(mfrow = c(3, 4), mar = c(4, 4, 2, 1))  # 4x4 grid
for (i in 1:ncol(ZLV)) {
  plot(BETA_ZLV[, i], type = "l", main = paste("Trace Plot for BETA_ZLV", variable_names[i]), 
       xlab = "Iteration", ylab = variable_names[i])
  plot(density(BETA_ZLV[, i]), main = paste("Density for BETA_ZLV", variable_names[i]), 
       xlab = variable_names[i], ylab = "Density")
}
dev.off()
```






```{r}


# Removing Initial Samples and Applying Thinning
BETA <- BETA[-c(1:20000), ]
ZLV <- ZLV[-c(1:20000), ]

thin_interval <- 60
BETA_thinned <- BETA[seq(1, nrow(BETA), by = thin_interval), ]
ZLV_thinned <- ZLV[seq(1, nrow(ZLV), by = thin_interval), ]

# Combining BETA and ZLV for Plotting
BETA_ZLV_thinned <- BETA_thinned * ZLV_thinned

# Generate Trace, Density, and ACF plots for BETA
pdf("Beta_Trace_Density_ACF_After_Thinning_DSSM2.pdf", width = 14, height = 10)
par(mfrow = c(3, 4), mar = c(4, 4, 2, 1))
for (i in 1:ncol(BETA_thinned)) {
  plot(BETA_thinned[, i], type = "l", main = paste("Trace Plot for BETA_thinned", variable_names[i]), xlab = "Iteration")
  plot(density(BETA_thinned[, i]), main = paste("Density for BETA_thinned", variable_names[i]), xlab = variable_names[i])
  acf(BETA_thinned[, i], main = paste("ACF for BETA_thinned", variable_names[i]))
}
dev.off()


# Generate Trace, Density, and ACF plots for BETA
pdf("Beta_zlv_Trace_Density_ACF_After_Thinning_DSSM2.pdf", width = 14, height = 10)
par(mfrow = c(3, 4), mar = c(4, 4, 2, 1))
for (i in 1:ncol(BETA_ZLV_thinned)) {
  plot(BETA_ZLV_thinned[, i], type = "l", main = paste("Trace Plot for BETA_ZLV_thinned", variable_names[i]), xlab = "Iteration")
  plot(density(BETA_ZLV_thinned[, i]), main = paste("Density for BETA_ZLV_thinned", variable_names[i]), xlab = variable_names[i])
  acf(BETA_ZLV_thinned[, i], main = paste("ACF for BETA_ZLV_thinned", variable_names[i]))
}
dev.off()
```

```{r}
theta_samples <- GLOBAL_THETA

plot(theta_samples, type = "l", main = expression("Trace Plot of " * theta), xlab = "Iteration", ylab = expression(theta))

```


```{r}
# Trace plot for global sparsity parameter θ
plot(GLOBAL_THETA, type = "l",
     main = expression("Trace Plot of Global " * theta),
     xlab = "Iteration", ylab = expression(theta))

# Density plot
plot(density(GLOBAL_THETA), main = expression("Density of Global " * theta))

```

```{r}
png("trace_global_theta.png", width = 800, height = 600)
plot(GLOBAL_THETA, type = "l",
     main = expression("Trace Plot of Global " * theta),
     xlab = "Iteration", ylab = expression(theta))
dev.off()

```
```{r}
png("density_global_theta.png", width = 800, height = 600)
plot(density(GLOBAL_THETA),
     main = expression("Density of Global " * theta),
     xlab = expression(theta), ylab = "Density")
dev.off()

```

```{r}
effectiveSize(GLOBAL_THETA)
acf(GLOBAL_THETA)
png("acf_global_theta.png", width = 800, height = 600)
acf(GLOBAL_THETA, main = "ACF of GLOBAL_THETA")
dev.off()

```


```{r}
theta_samples <- THETA

plot(theta_samples, type = "l", main = expression("Trace Plot of " * theta), xlab = "Iteration", ylab = expression(theta))

```

```{r}
library(coda)

# Example: for beta coefficients
mcmc_beta <- as.mcmc(BETA)
mcmc_theta <- as.mcmc(THETA)
mcmc_global_theta <- as.mcmc(GLOBAL_THETA)

# Combine if you want a full object
mcmc_combined <- mcmc(cbind(BETA[, 1:3], GLOBAL_THETA))  # Just 3 beta_j + global theta

```


```{r}
# Use summary to get diagnostics
summary(mcmc_beta)
summary(mcmc_global_theta)

```





```{r}

# Ensure we have names for each column in BETA and ZLV
colnames(BETA) <- variable_names
colnames(ZLV) <- variable_names

# 1. Effective Sample Size (ESS) for `BETA` and `ZLV`
ess_beta_thinned <- effectiveSize(as.mcmc(BETA_thinned))
ess_zlv_thinned <- effectiveSize(as.mcmc(ZLV_thinned))
ess_beta_zlv_thinned<-effectiveSize(as.mcmc(BETA_ZLV_thinned))
# Display ESS for both in a summary table
ess_summary <- data.frame(
  Parameter = variable_names,
  ESS_Beta_thinned = round(ess_beta, 2),
  ESS_ZLV_thinned = round(ess_zlv, 2),
  ESS_ZLV_thinned = round(ess_beta_zlv,2)
)

# Save ESS Summary as a PDF
pdf("ESS_Summary_Beta_ZLV_DSSM2.pdf", width = 8, height = 5)
grid.table(ess_summary)
dev.off()

```








```{r}
# Top 5 Models Analysis
ZLV_list <- as.list(as.data.frame(ZLV))
top5_models <- do.call(paste0, ZLV_list) %>% table() %>% sort(decreasing = TRUE) %>% .[1:5]
top5_probabilities <- top5_models / sum(top5_models)

top5_df <- data.frame(
  Model = paste0("Model ", 1:5),
  Probability = round(as.numeric(top5_probabilities), 4),
  Variables = names(top5_probabilities)
)

# Plotting Posterior Probabilities of the Top 5 Models
plot <- ggplot(top5_df, aes(x = Model, y = Probability)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Probability, 2)), vjust = -0.5) +
  labs(title = "Posterior Probabilities of Top 5 Models-DSS- Flexible", x = "Model", y = "Posterior Probability") +
  theme_minimal()

ggsave("posterior_probabilities_top5_models_DSSM2.pdf", plot = plot, width = 10, height = 6)

# Creating Table for the Top 5 Models
table_grob <- tableGrob(top5_df)
pdf("top5_models_table_OA2.pdf", width = 8, height = 6)
grid.draw(table_grob)
dev.off()
```



```{r}


# Define the variable names 
variable_names <- colnames(X_train) 

# Assign these names to the columns of BETA_thinned
colnames(BETA_thinned) <- variable_names
colnames(BETA_ZLV_thinned) <- variable_names

# Calculate Mean Beta Values from Thinned MCMC Samples
beta_means <- colMeans(BETA_ZLV_thinned)
names(beta_means) <- variable_names
```

```{r}


# Define Helper Function for Credible Intervals
calculate_quantiles <- function(beta_matrix, probs = c(0.025, 0.5, 0.975)) {
  apply(beta_matrix, 2, quantile, probs = probs)
}

# Calculate Credible Intervals from BETA_ZLV_thinned
beta_quantiles <- calculate_quantiles(BETA_ZLV_thinned)
beta_means <- colMeans(BETA_ZLV_thinned)
variable_names <- colnames(BETA_ZLV_thinned)  # Extract variable names dynamically

# Prepare Data for Plotting Credible Intervals
beta_df <- as.data.frame(t(beta_quantiles))
beta_df$Variable <- variable_names
beta_df <- beta_df[, c("Variable", "2.5%", "50%", "97.5%")]

# Identify Significant Predictors Based on Credible Intervals
significant_predictors <- variable_names[apply(beta_quantiles, 2, function(x) !(x[1] < 0 & x[3] > 0))]
beta_df$Significance <- ifelse(beta_df$Variable %in% significant_predictors, "Significant", "Non-significant")

# Exclude "Intercept" ONLY for Plotting
beta_df <- beta_df[beta_df$Variable != "(Intercept)", ]

p1 <- ggplot(beta_df, aes(x = reorder(Variable, `50%`), y = `50%`, color = Significance)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = `2.5%`, ymax = `97.5%`), width = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  labs(
    title = "Credible Interval- Flexible Prior- DSS",
    x = "Predictors",
    y = "Posterior Mean"
  ) +
  scale_color_manual(
    values = c("Significant" = "black", "Non-significant" = "gray"),
    name = "Significance"
  ) +
  coord_flip() +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 14, face = "bold", color = "black"), # Larger, bolder text for y-axis labels
    axis.text.x = element_text(size = 12), # Adjust x-axis text size if needed
    legend.title = element_text(size = 12, face = "bold"),
    legend.text = element_text(size = 10)
  )

# Calculate Posterior Inclusion Probabilities (PIP) from ZLV_thinned
pip <- colMeans(ZLV_thinned)
pip_df <- data.frame(Variable = variable_names, PIP = pip)
pip_df$Significance <- ifelse(pip_df$Variable %in% significant_predictors, "Significant", "Non-significant")

# Exclude "Intercept" from PIP Data
pip_df <- pip_df[pip_df$Variable != "(Intercept)", ]

# Plot PIP Values
p2 <- ggplot(pip_df, aes(x = reorder(Variable, PIP), y = PIP, fill = Significance)) +
  geom_bar(stat = "identity", width = 0.8) +
  scale_fill_manual(
    values = c("Significant" = "black", "Non-significant" = "gray"),
    name = "Significance"
  ) +
  coord_flip() +
  labs(
    title = "PIP- Flexible Prior- DSS",
    x = "Predictors",
    y = "PIP"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 14, face = "bold", color = "black"), # Larger, bolder text for y-axis labels
    axis.text.x = element_text(size = 12), # Adjust x-axis text size if needed
    legend.title = element_text(size = 12, face = "bold"),
    legend.text = element_text(size = 10)
  )

# Save Combined Plots as Side-by-Side
pdf("Credible_Interval_and_PIP_Plots_DSSM2.pdf", width = 12, height = 8)
grid.arrange(p1, p2, ncol = 2)
dev.off()

# Print Plots
print(p1)
print(p2)


```

```{r}
# Load Required Libraries
library(ggplot2)
library(gridExtra)
# Identify Significant Predictors Based on Credible Intervals
significant_predictors <- variable_names[apply(beta_quantiles, 2, function(x) !(x[1] < 0 & x[3] > 0))]
beta_df$Significance <- ifelse(beta_df$Variable %in% significant_predictors, "Significant", "Non-significant")

# Exclude "Intercept" ONLY for Plotting
beta_df <- beta_df[beta_df$Variable != "(Intercept)", ]
# Adjust Plot for Credible Intervals
p1 <- ggplot(beta_df, aes(x = reorder(Variable, `50%`), y = `50%`, color = Significance)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = `2.5%`, ymax = `97.5%`), width = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  labs(
    title = "Credible Interval- Flexible Prior- DSS_50",
    x = "Predictors",
    y = "Posterior Mean"
  ) +
  scale_color_manual(
    values = c("Significant" = "black", "Non-significant" = "gray"),
    name = "Significance"
  ) +
  coord_flip() +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 8, hjust = 1), # Reduced font size for y-axis
    axis.text.x = element_text(size = 8), # Reduced font size for x-axis
    legend.title = element_text(size = 10, face = "bold"),
    legend.text = element_text(size = 8) # Reduced legend text size
  )
# Calculate Posterior Inclusion Probabilities (PIP) from ZLV_thinned
pip <- colMeans(ZLV_thinned)
pip_df <- data.frame(Variable = variable_names, PIP = pip)
pip_df$Significance <- ifelse(pip_df$Variable %in% significant_predictors, "Significant", "Non-significant")

# Exclude "Intercept" from PIP Data
pip_df <- pip_df[pip_df$Variable != "(Intercept)", ]
# Adjust Plot for Posterior Inclusion Probabilities (PIP)
p2 <- ggplot(pip_df, aes(x = reorder(Variable, PIP), y = PIP, fill = Significance)) +
  geom_bar(stat = "identity", width = 0.8) +
  scale_fill_manual(
    values = c("Significant" = "black", "Non-significant" = "gray"),
    name = "Significance"
  ) +
  coord_flip() +
  labs(
    title = "PIP- Flexible Prior- DSS_50",
    x = "Predictors",
    y = "PIP"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 8, hjust = 1), # Reduced font size for y-axis
    axis.text.x = element_text(size = 8), # Reduced font size for x-axis
    legend.title = element_text(size = 10, face = "bold"),
    legend.text = element_text(size = 8) # Reduced legend text size
  )

# Save Combined Plots as Side-by-Side
pdf("Credible_Interval_and_PIP_Plots_DSSM2_50_Adjusted.pdf", width = 16, height = 10)  # Increased plot size
grid.arrange(p1, p2, ncol = 2)
dev.off()

# Print Plots
print(p1)
print(p2)

```


```{r}
# Calculate Posterior Inclusion Probabilities (PIP) from ZLV_thinned
pip <- colMeans(ZLV_thinned)
pip_df <- data.frame(Variable = variable_names, PIP = pip)
pip_df$Significance <- ifelse(pip_df$Variable %in% significant_predictors, "Significant", "Non-significant")
# Exclude "Intercept" from both beta_df and pip_df
beta_df <- beta_df[beta_df$Variable != "(Intercept)", ]
pip_df <- pip_df[pip_df$Variable != "(Intercept)", ]

# Create the PIP plot first to ensure the order is correct
p2 <- ggplot(pip_df, aes(x = reorder(Variable, PIP), y = PIP, fill = Significance)) +
  geom_bar(stat = "identity", width = 0.8) +
  scale_fill_manual(
    values = c("Significant" = "black", "Non-significant" = "gray"),
    name = "Significance"
  ) +
  coord_flip() +
  labs(
    title = "Posterior selection probability",
    x = " ",
    y = " "
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12, face = "bold"),
    axis.text.y =element_blank(),
    #axis.text.y = element_text(size = 8, hjust = 1), # Reduced font size for y-axis
    axis.text.x = element_text(size = 8), # Reduced font size for x-axis
    legend.position = "none" # Remove legend
  )

# Extract the order of variables from the PIP plot
pip_order <- levels(reorder(pip_df$Variable, pip_df$PIP))

# Reorder the Variable column in beta_df to match the order in the PIP plot
beta_df$Variable <- factor(beta_df$Variable, levels = pip_order)

# Plot Credible Intervals with variables ordered by decreasing PIP values
p1 <- ggplot(beta_df, aes(x = Variable, y = `50%`, color = Significance)) +
  geom_point(size = 3, color = "darkgray") +
  geom_errorbar(aes(ymin = `2.5%`, ymax = `97.5%`), width = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  labs(
    title = "Credible interval",
    x = " ",
    y = " "
  ) +
  scale_color_manual(
    values = c("Significant" = "black", "Non-significant" = "gray"),
    name = "Significance"
  ) +
  coord_flip() +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 8, hjust = 1), # Reduced font size for y-axis
    axis.text.x = element_text(size = 8), # Reduced font size for x-axis
    legend.position = "none" # Remove legend
  )

# Save the Credible Interval plot
ggsave("Credible_interval_plot_DSSM2.pdf", plot = p1, width = 8, height = 6)

# Save Combined Plots as Side-by-Side
pdf("Credible_Interval_and_PIP_Plots_DSSM2.pdf", width = 12, height = 8)
grid.arrange(p1, p2, ncol = 2)
dev.off()

pdf("Credible_Interval_and_PIP_Plots_widthDSSM2.pdf", width = 12, height = 8)
grid.arrange(p1, p2, ncol = 2, widths = c(0.65, 0.35))
dev.off()
# Print Plots
print(p1)
print(p2)
```


```{r}
significant_predictors
```
```{r}
# Filter for Significant Genes
significant_genes <- beta_df$Variable[beta_df$Significance == "Significant"]

# Classify Positively and Negatively Associated Significant Genes
positively_associated_genes <- significant_genes[beta_df$`50%`[beta_df$Significance == "Significant"] > 0]
negatively_associated_genes <- significant_genes[beta_df$`50%`[beta_df$Significance == "Significant"] < 0]

# Print the Results
cat("Significant Positively Associated Genes:\n")
print(positively_associated_genes)
```

```{r}

# Load Required Libraries
library(ggplot2)
library(gridExtra)

# Define Helper Function for Credible Intervals
calculate_quantiles <- function(beta_matrix, probs = c(0.025, 0.5, 0.975)) {
  apply(beta_matrix, 2, quantile, probs = probs)
}

# Calculate Credible Intervals from BETA_ZLV_thinned
beta_quantiles <- calculate_quantiles(BETA_ZLV_thinned)
beta_means <- colMeans(BETA_ZLV_thinned)
variable_names <- colnames(BETA_ZLV_thinned)  # Extract variable names dynamically

# Prepare Data for Credible Intervals
beta_df <- as.data.frame(t(beta_quantiles))
beta_df$Variable <- variable_names
beta_df <- beta_df[, c("Variable", "2.5%", "50%", "97.5%")]

# Identify Significant Predictors Based on Credible Intervals
significant_predictors <- variable_names[apply(beta_quantiles, 2, function(x) !(x[1] < 0 & x[3] > 0))]
beta_df$Significance <- ifelse(beta_df$Variable %in% significant_predictors, "Significant", "Non-significant")

# Calculate Posterior Inclusion Probabilities (PIP) from ZLV_thinned
pip <- colMeans(ZLV_thinned)
pip_df <- data.frame(Variable = variable_names, PIP_Flexible = pip)
pip_df$Significance <- ifelse(pip_df$Variable %in% significant_predictors, "Significant", "Non-significant")

# Combine Credible Intervals and PIP into a Single DataFrame
combined_df <- merge(beta_df, pip_df, by = "Variable", suffixes = c("_CI", "_PIP"))

# Compute Width of the 95% Credible Interval for Fixed Prior
combined_df$Width_Flexible <- combined_df$`97.5%` - combined_df$`2.5%`

# Select relevant columns
combined_df <- combined_df[, c("Variable", "2.5%", "50%", "97.5%", "Width_Flexible", "Significance_CI", "PIP_Flexible", "Significance_PIP")]

# Exclude "Intercept" from the Combined DataFrame
combined_df <- combined_df[combined_df$Variable != "(Intercept)", ]

# Print Combined DataFrame
print(combined_df)

# Save Combined DataFrame as CSV
write.csv(combined_df, "Predictor_Significance_and_PIP_Flexible.csv", row.names = FALSE)

# Visualize DataFrame as a Table
pdf("Significance_and_PIP_Table_Flexible.pdf", width = 12, height = 8)
grid.table(combined_df)
dev.off()
```




```{r}

# Load Required Libraries
library(ggplot2)
library(gridExtra)

# Define Helper Function for Credible Intervals
calculate_quantiles <- function(beta_matrix, probs = c(0.025, 0.5, 0.975)) {
  apply(beta_matrix, 2, quantile, probs = probs)
}

# Calculate Credible Intervals from BETA_ZLV_thinned
beta_quantiles <- calculate_quantiles(BETA_ZLV_thinned)
beta_means <- colMeans(BETA_ZLV_thinned)
variable_names <- colnames(BETA_ZLV_thinned)  # Extract variable names dynamically

# Prepare Data for Credible Intervals
beta_df <- as.data.frame(t(beta_quantiles))
beta_df$Variable <- variable_names
beta_df <- beta_df[, c("Variable", "2.5%", "50%", "97.5%")]

# Identify Significant Predictors Based on Credible Intervals
significant_predictors <- variable_names[apply(beta_quantiles, 2, function(x) !(x[1] < 0 & x[3] > 0))]
beta_df$Significance <- ifelse(beta_df$Variable %in% significant_predictors, "Significant", "Non-significant")

# Calculate Posterior Inclusion Probabilities (PIP) from ZLV_thinned
pip <- colMeans(ZLV_thinned)
pip_df <- data.frame(Variable = variable_names, PIP = pip)
pip_df$Significance <- ifelse(pip_df$Variable %in% significant_predictors, "Significant", "Non-significant")

# Combine Credible Intervals and PIP into a Single DataFrame
combined_df <- merge(beta_df, pip_df, by = "Variable", suffixes = c("_CI", "_PIP"))
combined_df <- combined_df[, c("Variable", "2.5%", "50%", "97.5%", "Significance_CI", "PIP", "Significance_PIP")]

# Exclude "Intercept" from the Combined DataFrame
combined_df <- combined_df[combined_df$Variable != "(Intercept)", ]

# Print Combined DataFrame
print(combined_df)

# Save Combined DataFrame as CSV
write.csv(combined_df, "Predictor_Significance_and_PIP_DSSM2.csv", row.names = FALSE)

# Visualize DataFrame as a Table
library(gridExtra)
pdf("Significance_and_PIP_Table_DSSM2.pdf", width = 12, height = 8)
grid.table(combined_df)
dev.off()
```

```{r}

# Load Required Libraries
library(ggplot2)
library(gridExtra)

# Define Helper Function for Credible Intervals
calculate_quantiles <- function(beta_matrix, probs = c(0.025, 0.5, 0.975)) {
  apply(beta_matrix, 2, quantile, probs = probs)
}

# Calculate Credible Intervals from BETA_ZLV_thinned
beta_quantiles <- calculate_quantiles(BETA_ZLV_thinned)
beta_means <- colMeans(BETA_ZLV_thinned)
variable_names <- colnames(BETA_ZLV_thinned)  # Extract variable names dynamically

# Prepare Data for Credible Intervals
beta_df <- as.data.frame(t(beta_quantiles))
beta_df$Variable <- variable_names
beta_df <- beta_df[, c("Variable", "2.5%", "50%", "97.5%")]

# Identify Significant Predictors Based on Credible Intervals
significant_predictors <- variable_names[apply(beta_quantiles, 2, function(x) !(x[1] < 0 & x[3] > 0))]
beta_df$Significance <- ifelse(beta_df$Variable %in% significant_predictors, "Significant", "Non-significant")

# Calculate Posterior Inclusion Probabilities (PIP) from ZLV_thinned
pip <- colMeans(ZLV_thinned)
pip_df <- data.frame(Variable = variable_names, PIP_Flexible= pip)
pip_df$Significance <- ifelse(pip_df$Variable %in% significant_predictors, "Significant", "Non-significant")

# Combine Credible Intervals and PIP into a Single DataFrame
combined_df <- merge(beta_df, pip_df, by = "Variable", suffixes = c("_CI", "_PIP"))

# Compute Width of the 95% Credible Interval for Fixed Prior
combined_df$Width_Flexible <- combined_df$`97.5%` - combined_df$`2.5%`

# Select relevant columns
combined_df <- combined_df[, c("Variable", "2.5%", "50%", "97.5%", "Width_Flexible", "Significance_CI", "PIP_Flexible", "Significance_PIP")]

# Exclude "Intercept" from the Combined DataFrame
combined_df <- combined_df[combined_df$Variable != "(Intercept)", ]

# Print Combined DataFrame
print(combined_df)

# Save Combined DataFrame as CSV
write.csv(combined_df, "Predictor_Significance_and_PIP_Flexible.csv", row.names = FALSE)

# Visualize DataFrame as a Table
pdf("Significance_and_PIP_Table_Flexible.pdf", width = 12, height = 8)
grid.table(combined_df)
dev.off()
```

```{r,eval=FALSE}
# Filter for Significant Predictors
significant_df <- combined_df[combined_df$Significance_CI == "Significant", ]

# Compute the credible interval width
significant_df$Interval_Width <- significant_df$`97.5%` - significant_df$`2.5%`

# Add Posterior Means
significant_df$Posterior_Mean <- beta_means[match(significant_df$Variable, variable_names)]

# Select Relevant Columns
significant_df <- significant_df[, c("Variable", "2.5%", "50%", "97.5%", "Interval_Width", "Posterior_Mean", "PIP")]

# Print the dataframe
print(significant_df)

# Save as CSV
write.csv(significant_df, "Significant_Predictors_DSSM2.csv", row.names = FALSE)

```


```{r}
# Compute posterior mean and 95% credible interval for global θ
theta_mean <- mean(GLOBAL_THETA)
theta_ci <- quantile(GLOBAL_THETA, probs = c(0.025, 0.5, 0.975))

# Print results
cat("Posterior Mean of Global θ:", theta_mean, "\n")
cat("95% Credible Interval for Global θ:", theta_ci, "\n")

```


```{r}
# Convert GLOBAL_THETA samples to a dataframe for plotting
theta_df <- data.frame(GlobalTheta = GLOBAL_THETA)




# Export to CSV
write.csv(theta_df, "bc_disease_theta.csv", row.names = FALSE)

```


```{r}
# Plot posterior distribution of Global θ
p_theta <- ggplot(theta_df, aes(x = GlobalTheta)) +
  geom_density(fill = "blue", alpha = 0.5) +
  geom_vline(xintercept = theta_ci[1], linetype = "dashed", color = "red") +
  geom_vline(xintercept = theta_ci[3], linetype = "dashed", color = "red") +
  geom_vline(xintercept = theta_mean, linetype = "solid", color = "black") +
  labs(title = " ", x = "Global θ", y = " ") +
  theme_minimal()

# Save the plot
ggsave("posterior_theta_distribution_gene.pdf", plot = p_theta, width = 8, height = 6)

# Print the plot
print(p_theta)

```

```{r}
# Create a data frame with global theta summary for each disease
theta_summary <- data.frame(
  Disease = c("Heart Disease", "Breast Cancer"),
  Mean = c(0.88, 0.736),
  Lower = c(0.75, 0.62),
  Upper = c(0.95, 0.82)
)

# Plot horizontal credible intervals for both
library(ggplot2)

theta_ci_plot <- ggplot(theta_summary, aes(y = Disease, x = Mean)) +
  geom_point(size = 3) +
  geom_errorbarh(aes(xmin = Lower, xmax = Upper), height = 0.2, size = 1) +
  labs(
    x = expression(Global~theta),
    y = "",
    title = ""
  ) +
  theme_minimal() +
  theme(
    axis.text = element_text(size = 12),
    axis.title.x = element_text(size = 14, face = "bold")
  )

# Save and display the plot
ggsave("theta_credible_intervals_comparison.pdf", plot = theta_ci_plot, width = 7, height = 4)
print(theta_ci_plot)

```

```{r}
# Create the data frame
theta_summary <- data.frame(
  Disease = c("Coronary artery disease", "Breast Cancer"),
  Mean = c(0.88, 0.736),
  Lower = c(0.75, 0.62),
  Upper = c(0.95, 0.82)
)

# Plot with Disease on X and Theta on Y
theta_ci_plot <- ggplot(theta_summary, aes(x = Disease, y = Mean)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.2, size = 1) +
  labs(
    y = expression(Global~theta),
    x = "",
    title = ""
  ) +coord_cartesian(ylim = c(0.6, 1.3)) +
  theme_minimal() +
 theme(
  axis.title.y = element_text(size = 14, face = "bold"),
  axis.text.x = element_text(size = 14, face = "bold"),
  axis.text.y = element_text(size = 12),
  plot.title = element_text(size = 16, face = "bold", hjust = 0.5)
)

# Save and display the plot
ggsave("theta_credible_intervals_vertical.pdf", plot = theta_ci_plot, width = 8, height = 6)
print(theta_ci_plot)

```

```{r}
# Evaluating the Model on Validation and Test Data 
predict_proba <- function(X, beta, zlv) {
  plogis(X %*% (beta * zlv))
}

beta_mean <- colMeans(BETA_thinned)
zlv_mean <- colMeans(ZLV_thinned)

train_preds <- predict_proba(X_train, beta_mean, zlv_mean)
#valid_preds <- predict_proba(X_valid, beta_mean, zlv_mean)
test_preds <- predict_proba(X_test, beta_mean, zlv_mean)

train_preds_binary <- ifelse(train_preds > 0.5, 1, 0)
#valid_preds_binary <- ifelse(valid_preds > 0.5, 1, 0)
test_preds_binary <- ifelse(test_preds > 0.5, 1, 0)

# Calculating Performance Metrics
calculate_metrics <- function(true_labels, pred_probs, pred_labels) {
  roc_curve <- roc(true_labels, pred_probs)
  auc <- auc(roc_curve)
  confusion <- confusionMatrix(factor(pred_labels), factor(true_labels))
  
  accuracy <- confusion$overall['Accuracy']
  f1 <- confusion$byClass['F1']
  sensitivity <- confusion$byClass['Sensitivity']
  specificity <- confusion$byClass['Specificity']
  precision <- confusion$byClass['Precision']
  error_rate <- 1 - accuracy
  fpr <- 1 - specificity
  fnr <- 1 - sensitivity
  tnr <- specificity
  
  list(
    Accuracy = accuracy,
    F1_Score = f1,
    Sensitivity = sensitivity,
    Specificity = specificity,
    Precision = precision,
    Error_Rate = error_rate,
    FPR = fpr,
    FNR = fnr,
    TNR = tnr,
    AUC = auc
  )
}

train_metrics <- calculate_metrics(y_train, train_preds, train_preds_binary)
#valid_metrics <- calculate_metrics(y_valid, valid_preds, valid_preds_binary)
test_metrics <- calculate_metrics(y_test, test_preds, test_preds_binary)

metrics_df <- data.frame(
  Metric = names(train_metrics),
  Train = unlist(train_metrics),
  #Validation = unlist(valid_metrics),
  Test = unlist(test_metrics)
)

pdf("performance_metrics_comparison_BVS_DSSM2.pdf", width = 8, height = 11)
grid.table(metrics_df)
dev.off()

```




```{r}
# Compute log-likelihood using final posterior means
loglik <- flogis.log(beta_mean, zlv_mean, X_train, y_train)

# Sample size
N <- nrow(X_train)

# Total number of predictors (excluding intercept)
m <- ncol(X_train) - 1

# Number of selected predictors (excluding intercept)
selected_predictors <- sum(zlv_mean[-1] > 0.5)

# Hierarchical BIC formula
bic_hier <- -2 * loglik + selected_predictors * log(N) + (m + 1) * log(m)

# Output
cat("Hierarchical BIC:", round(bic_hier, 2), "\n")


```

```{r}
# Step 1: Deviance at each MCMC iteration
loglik_samples <- numeric(nrow(BETA_thinned))
for (i in 1:nrow(BETA_thinned)) {
  loglik_samples[i] <- flogis.log(BETA_thinned[i, ], ZLV_thinned[i, ], X_train, y_train)
}
deviance_samples <- -2 * loglik_samples

# Step 2: Posterior mean deviance
D_bar <- mean(deviance_samples)

# Step 3: Compute deviance at posterior mean
beta_mean <- colMeans(BETA_thinned)
zlv_mean <- colMeans(ZLV_thinned)
D_hat <- -2 * flogis.log(beta_mean, zlv_mean, X_train, y_train)

# Step 4: Effective number of parameters
p_D <- D_bar - D_hat

# Step 5: Final DIC
DIC <- D_bar + p_D
cat("DIC:", round(DIC, 2), "\n")

```

```{r}
# Step 1: Compute log-likelihood at each MCMC iteration
loglik_samples <- numeric(nrow(BETA_thinned))
for (i in 1:nrow(BETA_thinned)) {
  loglik_samples[i] <- flogis.log(BETA_thinned[i, ], ZLV_thinned[i, ], X_train, y_train)
}
deviance_samples <- -2 * loglik_samples

# Step 2: Posterior mean deviance
D_bar <- mean(deviance_samples)

# Step 3: Compute deviance at posterior mean of (beta * Z)
# Note: this is the key correction — combining them first, then averaging
beta_zlv_mean <- colMeans(BETA_thinned * ZLV_thinned)

# Step 4: Evaluate deviance at this effective mean parameter vector
# Set Z vector to 1 because you've already zeroed out coefficients
D_hat <- -2 * flogis.log(beta_zlv_mean, rep(1, length(beta_zlv_mean)), X_train, y_train)

# Step 5: Effective number of parameters
p_D <- D_bar - D_hat

# Step 6: Compute DIC
DIC <- D_bar + p_D

# Step 7: Print result
cat("Posterior mean deviance (D̄):", round(D_bar, 2), "\n")
cat("Deviance at mean (D̂):", round(D_hat, 2), "\n")
cat("Effective number of parameters (pD):", round(p_D, 2), "\n")
cat("Deviance Information Criterion (DIC):", round(DIC, 2), "\n")

```

```{r}
beta_mean <- colMeans(BETA_thinned)
zlv_mean <- colMeans(ZLV_thinned)
zlv_binary <- round(zlv_mean)

loglik <- flogis.log(beta_mean, zlv_binary, X_train, y_train)

n <- nrow(X_train)
m <- ncol(X_train)
k_sel <- sum(zlv_binary)

BIC_flex <- -2 * loglik +
             k_sel * log(n) +
             (m + 1) * log(m)

cat("BIC (Flexible):", round(BIC_flex, 2))

```



