---
title: "CAD_Beta(4,4)"
author: "Ayisha - TRU- T00727585"
date: "2024-11-12"
output: html_document
---

```{r}


# ─────────────────────────────────────────────
# 📌 SECTION: Load Required Libraries
# Description:
# This block loads all the R packages needed for:
# - Bayesian analysis
# - Data manipulation
# - Plotting and reporting
# - MCMC diagnostics
# ─────────────────────────────────────────────

# Global options for knitr
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

# Core statistical packages
library(ISLR)        # Datasets and examples
library(stats4)      # Statistical modeling
library(MASS)        # Statistical functions and distributions
library(Matrix)      # Efficient matrix computations
library(mvtnorm)     # Multivariate normal sampling

# Data manipulation
library(dplyr)       # pspe-friendly data manipulation
library(tidyr)       # Data tidying
library(readr)       # Fast CSV reading

# MCMC sampling and diagnostics
library(MCMCpack)    # MCMC sampling tools
library(coda)        # MCMC diagnostics

# Visualization
library(ggplot2)     # Grammar of graphics
library(gridExtra)   # Arrange multiple plots
library(grid)        # Grid graphics system

# Model reporting
library(stargazer)   # Export regression tables

# Evaluation
library(pROC)        # ROC curves and AUC
library(caret)       # Model evaluation (confusion matrix, etc.)

# (Deprecated) Reshaping – used in legacy parts of script
library(reshape)     
```

```{r}
# Load data
trainData <- read.csv("training_data_tt.csv", header = TRUE, stringsAsFactors = FALSE)
testData <- read.csv("test_data_tt.csv", header = TRUE, stringsAsFactors = FALSE)

# Define logistic log-likelihood function
flogis.log <- function(beta, zlv, X, y) {
  p <- plogis(X %*% (beta * zlv))
  sum(ifelse(y, log(p), log(1 - p)))
}

# Prepare data for analysis, excluding the intercept
y_train <- trainData[, "DSS"]
X_train <- model.matrix(as.formula(paste("DSS ~ .")), data = trainData)  
y_test <- testData[, "DSS"]
X_test <- model.matrix(as.formula(paste("DSS ~ . ")), data = testData)

# Update k to reflect the new number of columns without the intercept
k <- ncol(X_train)
n <- nrow(X_train)


```


```{r}
# Step 1: Performing Frequentist Logistic Regression for Initial Beta Estimates
reg.glm <- glm(DSS ~ .  , data = trainData, family = binomial)
BETA_init <- as.vector(reg.glm$coef)
vcov_matrix <- vcov(reg.glm)


# Step 2: Calculating the Prior Mean and Standard Deviation for Beta
pmn.beta <- BETA_init
psd.beta <- vcov_matrix

```


```{r}
# Initial setup for MCMC
S <- 150000  # Number of MCMC iterations
c <- 1       # Proposal variance tuning parameter
k <- ncol(X_train)  # Number of predictors (confirm this matches)
beta <- BETA_init
alpha1 <- 4
alpha2 <- 4
# Initialize zlv based on global theta
#global_theta <- rbeta(1, alpha1, alpha2)  # Initial global theta from Beta prior
global_theta<-0.5
theta_j <- rbeta(k, global_theta, 1 - global_theta)  # Initialize each theta_j from global theta

zlv <- rbinom(k, 1, theta_j)  # Initial inclusion/exclusion indicators for predictors

# Initialize matrices and counters
acsb <- acsg <- 0
BETA <- matrix(0, nrow = S, ncol = k)   # Store samples of beta for each iteration
ZLV <- matrix(0, nrow = S, ncol = k)    # Store samples of zlv for each iteration
THETA <- matrix(0, nrow = S, ncol = k)       # Store samples for each theta_j
accept_beta <- rep(0, k)                # Acceptance count for beta updates
accept_zlv <- rep(0, k)                 # Acceptance count for zlv updates
global_theta_values <- numeric(S)                  # Store samples for global theta
short_chain_length<-1000
# Confirm dimensions
print(dim(X_train))  # Should match (n, k)
print(length(beta))  # Should match k
print(length(zlv))   # Should match k
print(dim(vcov_matrix)) 
# Proposal Variance for MCMC
var.prop <- c * vcov_matrix  


```

```{r}
# Verify dimensions before starting MCMC tuning
cat("Checking dimensions...\n")

# beta vector (initial estimate)
cat("Length of beta: ", length(beta), "\n")

# vcov_matrix should be square matrix (k x k)
cat("Dimensions of vcov_matrix: ", dim(vcov_matrix), "\n")

# var.prop must be same as vcov_matrix (after scaling by c)
var.prop <- c * vcov_matrix
cat("Dimensions of var.prop: ", dim(var.prop), "\n")

# BETA_init should also be same length as beta
cat("Length of BETA_init: ", length(BETA_init), "\n")
```



```{r}
# ─────────────────────────────────────────────
# 📌 SECTION: Tune Proposal Variance (Scaling Factor c)
# Description:
# - Short-run Metropolis-Hastings updates for β
# - Tune the proposal scaling constant `c` for efficiency
# ─────────────────────────────────────────────

# Track acceptance rate
set.seed(1)  # For reproducibility

start_time <- Sys.time()
var.prop <- c * vcov_matrix
for (i in 1:10) {  # Adjust the proposal variance over 10 iterations of tuning
  acs <- 0  # Acceptance count
  
  for (s in 1:short_chain_length) {
    # Propose a new beta using current variance scale (c * vcov_matrix)
    beta.p <- t(rmvnorm(1, beta, c * vcov_matrix))
    
    # Calculate log acceptance ratio (log-hastings ratio)
    lhr <- flogis.log(beta.p, zlv, X_train, y_train) - flogis.log(beta, zlv, X_train, y_train) +
           dmvnorm(t(beta.p), BETA_init, vcov_matrix, log = TRUE) -
           dmvnorm(t(beta), BETA_init, vcov_matrix, log = TRUE)
    
    # Accept or reject based on acceptance ratio
    if (log(runif(1)) < lhr) {
      beta <- beta.p
      acs <- acs + 1
    }
  }
  
  # Calculate acceptance rate
  acceptance_rate <- acs / short_chain_length
  print(paste("Iteration", i, "- Acceptance rate:", acceptance_rate))
  
  # Adjust `c` based on acceptance rate
  if (acceptance_rate < 0.2) {
    c <- c / 2  # Reduce `c` if acceptance rate is too low
  } else if (acceptance_rate > 0.5) {
    c <- c * 2  # Increase `c` if acceptance rate is too high
  }
  
  # Print adjusted `c` value for tracking
  print(paste("Adjusted proposal variance scaling factor (c):", c))
}

# After tuning, `c` can be used in the main MCMC chain
print(paste("Final proposal variance scaling factor (c):", c))
```



```{r}



# clamp to (eps, 1-eps)
clamp01 <- function(x, eps = 1e-12) pmin(pmax(x, eps), 1 - eps)

# --- user inputs expected to exist -------------------------------------------
# X_train : n x k matrix of predictors including intercept
# y_train : length-n binary response (0/1)
# BETA_init : length-k vector 
# vcov_matrix: k x k covariance matrix 

# --- settings ----------------------------------------------------------------
set.seed(1)                      # reproducibility
#c        <- 0.125
S        <- 150000               # iterations
k        <- ncol(X_train)
beta     <- as.numeric(BETA_init)
alpha1   <- 4
alpha2   <- 4
eps      <- 1e-12                # small guard to avoid 0/1 boundaries
kappa    <- 20                   # Beta-proposal concentration for global_theta

# --- init --------------------------------------------------------------------
#global_theta <- rbeta(1, alpha1, alpha2) 
global_theta <- 0.5 # global sparsity
theta_j      <- rbeta(k, global_theta, 1 - global_theta) # local inclusion probs

# clamp immediately (prevents early NaN/-Inf downstream)
theta_j      <- clamp01(theta_j, eps)
global_theta <- clamp01(global_theta, eps)

# inclusion indicators
zlv <- rbinom(k, size=1, prob=theta_j)

# >>> save "true initial" values BEFORE any updates <<<
INITIAL_GLOBAL_THETA <- global_theta
INITIAL_THETA_J      <- theta_j
INITIAL_ZLV          <- zlv

# storage
BETA         <- matrix(0, nrow = S, ncol = k)
ZLV          <- matrix(0, nrow = S, ncol = k)
THETA        <- matrix(0, nrow = S, ncol = k)
GLOBAL_THETA <- numeric(S)

# acceptance counters
acsb <- 0L                      # beta acceptances
acsg <- 0L                      # zlv acceptances (one proposal per coord per sweep)
accept_global_theta <- 0L       # global_theta acceptances

# proposals for beta
var.prop <- c * vcov_matrix

# --- MCMC --------------------------------------------------------------------
for (s in 1:S) {

  # ---- Step 1: MH for beta ---------------------------------------------------
  beta.p <- as.numeric(rmvnorm(1, mean = beta, sigma = var.prop))
  lhr <- flogis.log(beta.p, zlv, X_train, y_train) -
         flogis.log(beta,   zlv, X_train, y_train) +
         dmvnorm(beta.p, mean = BETA_init, sigma = vcov_matrix, log = TRUE) -
         dmvnorm(beta,   mean = BETA_init, sigma = vcov_matrix, log = TRUE)

  if (log(runif(1)) < lhr) {
    beta <- beta.p
    acsb <- acsb + 1L
  }
  BETA[s, ] <- beta

  # ---- Step 2: MH for z (component-wise flips) -------------------------------
  # IMPORTANT FIX: clamp theta_j BEFORE using it in Bernoulli prior terms
  theta_j <- clamp01(theta_j, eps)

  for (j in 1:k) {
    zlv.p    <- zlv
    zlv.p[j] <- 1L - zlv[j]  # flip j-th indicator

    ll_diff <- flogis.log(beta, zlv.p, X_train, y_train) -
               flogis.log(beta, zlv,   X_train, y_train)

    # Bernoulli prior ratio for z_j | theta_j
    #lp_diff <- dbinom(zlv.p[j], size = 1, prob = theta_j[j], log = TRUE) -
               #dbinom(zlv[j],   size = 1, prob = theta_j[j], log = TRUE)
    lp_diff <- sum(dbinom(zlv.p, size = 1, prob = theta_j, log = TRUE)) -
               sum(dbinom(zlv,   size = 1, prob = theta_j, log = TRUE))

    lhg <- ll_diff + lp_diff

    if (log(runif(1)) < lhg) {
      zlv[j] <- zlv.p[j]
      acsg   <- acsg + 1L
    }
  }
  ZLV[s, ] <- zlv

  # ---- Step 3: Gibbs for local theta_j ---------------------------------------
  # Gibbs => accept prob = 1 (no acceptance rate needed/reported)
  for (j in 1:k) {
    theta_j[j] <- rbeta(1,
                        shape1 = global_theta + zlv[j],
                        shape2 = (1 - global_theta) + (1 - zlv[j]))
  }
  # IMPORTANT FIX: clamp theta_j AFTER Gibbs for stability in next steps
  theta_j    <- clamp01(theta_j, eps)
  THETA[s, ] <- theta_j

  # ---- Step 4: MH for global_theta (Beta-centered) ---------------------------
  # IMPORTANT FIX: clamp current θ and the proposal to avoid boundary explosions
  global_theta <- clamp01(global_theta, eps)

  a_prop <- global_theta * kappa
  b_prop <- (1 - global_theta) * kappa

  proposed_global_theta <- rbeta(1, a_prop, b_prop)
  proposed_global_theta <- clamp01(proposed_global_theta, eps)

  # target:
  #   π(θ) ∝ [∏_j Beta(theta_j ; θ, 1 - θ)] * Beta(θ ; alpha1, alpha2)
  log_num <- sum(dbeta(theta_j, proposed_global_theta,
                       1 - proposed_global_theta, log = TRUE)) +
             dbeta(proposed_global_theta, alpha1, alpha2, log = TRUE)

  log_den <- sum(dbeta(theta_j, global_theta,
                       1 - global_theta, log = TRUE)) +
             dbeta(global_theta, alpha1, alpha2, log = TRUE)

  # Hastings correction for asymmetric Beta proposal:
  # q(θ* | θ) = Beta(κθ, κ(1-θ)), q(θ | θ*) = Beta(κθ*, κ(1-θ*))
  log_q_fwd <- dbeta(proposed_global_theta, a_prop, b_prop, log = TRUE)
  log_q_rev <- dbeta(global_theta,
                     proposed_global_theta * kappa,
                     (1 - proposed_global_theta) * kappa, log = TRUE)

  lhr_theta <- (log_num - log_den) + (log_q_rev - log_q_fwd)

  # With proper clamping, lhr_theta should be finite; no need to skip updates
  if (log(runif(1)) < lhr_theta) {
    global_theta <- proposed_global_theta
    accept_global_theta <- accept_global_theta + 1L
  }
  GLOBAL_THETA[s] <- global_theta

  # ---- optional debug --------------------------------------------------------
  if (s %% 1000 == 0) {
    cat("it:", s,
        "  global_theta:", signif(global_theta, 5),
        "  beta[1]:", signif(beta[1], 5),
        "  sum(z):", sum(zlv), "\n")
  }
}

# --- acceptance rates ---------------------------------------------------------
cat("Acceptance rate (beta):           ", acsb / S, "\n")
cat("Acceptance rate (z per flip):     ", acsg / (S * k),
    "  # denominator = S*k proposals\n")
cat("Acceptance rate (global theta):   ", accept_global_theta / S, "\n")

```





```{r}
GLOBAL_THETA[1:10]

```


```{r}
# Save the results
save(BETA, ZLV,THETA,GLOBAL_THETA,theta_j, file = "BC-Proposed model_Beta(4,4).RData")

# To load the results back in future sessions
load("BC-Proposed model_Beta(4,4).RData")

```

```{r}
# Keep original variable names (from model matrix)
variable_names <- colnames(X_train)


# usage in plots/tables:
colnames(BETA) <- variable_names   # keep original names in the matrices
colnames(ZLV)  <- variable_names
```


```{r}
# When making a dataframe for display:
ess_summary <- data.frame(
  Parameter = variable_names,   
  ESS_Beta  = round(effectiveSize(as.mcmc(BETA)), 2),
  ESS_Z     = round(effectiveSize(as.mcmc(ZLV)), 2)
)

# When plotting with ggplot, replace axis labels
ggplot(ess_summary, aes(x = Parameter, y = ESS_Beta)) +
  geom_bar(stat = "identity") +
  labs(title = "Effective Sample Size", x = "Predictors", y = "ESS") +
  theme_minimal()

```




```{r}
# ─────────────────────────────────────────────
# 📌 SECTION: Burn-in, Thinning & Diagnostics
# Description:
# - Remove initial burn-in
# - Apply thinning to reduce autocorrelation
# - Recompute Effective Sample Size (ESS) post-thinning
# - Generate trace, density, and ACF plots (post-thinning only)
# ─────────────────────────────────────────────

# 1) Burn-in and thinning
burn_in       <- 50000       # discard first 20,000 iterations
thin_interval <- 50          # keep every 60th draw

BETA_thinned <- BETA[-seq_len(burn_in), , drop = FALSE]
ZLV_thinned  <- ZLV[-seq_len(burn_in),  , drop = FALSE]

BETA_thinned <- BETA_thinned[seq(1, nrow(BETA_thinned), by = thin_interval), , drop = FALSE]
ZLV_thinned  <- ZLV_thinned[seq(1, nrow(ZLV_thinned),  by = thin_interval),  , drop = FALSE]

# 2) ESS after thinning
ess_beta <- effectiveSize(as.mcmc(BETA_thinned))
ess_zlv  <- effectiveSize(as.mcmc(ZLV_thinned))

ess_summary <- data.frame(
  Parameter = variable_names,    # use human-readable names
  ESS_Beta  = round(ess_beta, 2),
  ESS_Z     = round(ess_zlv,  2)
)
print(ess_summary)

# Save as PDF table
pdf("ESS_Summary_PostThinning_flexible_beta_bc_(1,1).pdf", width = 8, height = 5)
grid.table(ess_summary, rows = NULL)
dev.off()

# 3) Diagnostics plots (post-thinning only)
BETA_ZLV_thinned <- BETA_thinned * ZLV_thinned

# Trace + Density plots
plot_trace_density_to_pdf <- function(param_matrix, param_names, file_name) {
  pdf(file_name, width = 8, height = 11)
  ncol_param <- ncol(param_matrix)
  for (i in seq_len(ncol_param)) {
    par(mfrow = c(1, 2), mar = c(4, 4, 3, 1))
    plot(param_matrix[, i], type = "l",
         main = paste("Trace:", param_names[i]),
         xlab = "Iteration (thinned)", ylab = "Value")
    plot(density(param_matrix[, i]),
         main = paste("Density:", param_names[i]),
         xlab = "Value", ylab = "Density")
  }
  dev.off()
}

# ACF plots
plot_acf_to_pdf <- function(param_matrix, param_names, file_name) {
  pdf(file_name, width = 8, height = 11)
  ncol_param <- ncol(param_matrix)
  par(mfrow = c(2, 2), mar = c(4, 4, 3, 1))
  for (i in seq_len(ncol_param)) {
    acf(param_matrix[, i], main = paste("ACF:", param_names[i]), xlab = "Lag", ylab = "ACF")
  }
  dev.off()
}

# Run diagnostics
plot_trace_density_to_pdf(BETA_thinned, variable_names, "TraceDensity_Beta_PostThinning_flexible_beta_bc_(1,1).pdf")
plot_trace_density_to_pdf(BETA_ZLV_thinned, variable_names, "TraceDensity_BetaZ_PostThinning_flexible_beta_bc_(1,1).pdf")
plot_acf_to_pdf(BETA_thinned, variable_names, "ACF_Beta_PostThinning_flexible_beta_bc_(1,1).pdf")
plot_acf_to_pdf(BETA_ZLV_thinned, variable_names, "ACF_BetaZ_PostThinning_flexible_beta_bc_(1,1).pdf")

```


```{r}

# Apply burn-in and thinning to GLOBAL_THETA
GLOBAL_THETA_thinned <- GLOBAL_THETA[-seq_len(burn_in)]
GLOBAL_THETA_thinned <- GLOBAL_THETA_thinned[seq(1, length(GLOBAL_THETA_thinned), by = thin_interval)]

# Posterior summary
theta_mean <- mean(GLOBAL_THETA_thinned)
theta_ci <- quantile(GLOBAL_THETA_thinned, c(0.025, 0.5, 0.975))
ess_theta <- effectiveSize(as.mcmc(GLOBAL_THETA_thinned))

# Print to console
cat("Posterior mean of global θ:", theta_mean)
cat("95% credible interval:", theta_ci)
cat("Effective sample size:", ess_theta)

# Trace + density plot for global θ
pdf("TraceDensity_GlobalTheta_PostThinning_flexible_beta_bc_(1,1).pdf", width = 8, height = 4)
par(mfrow = c(1, 2), mar = c(4, 4, 3, 1))
plot(GLOBAL_THETA_thinned, type = "l", main = "Trace: Global θ", xlab = "Iteration", ylab = expression(theta))
plot(density(GLOBAL_THETA_thinned), main = "Density: Global θ", xlab = expression(theta), ylab = "Density")
dev.off()

# ACF plot
pdf("ACF_GlobalTheta_PostThinning_flexible_beta_bc_(1,1).pdf", width = 6, height = 4)
acf(GLOBAL_THETA_thinned, main = "ACF: Global θ", xlab = "Lag", ylab = "ACF")
dev.off()


```





```{r}
print(global_theta)
```

```{r}
theta_samples <- GLOBAL_THETA

plot(theta_samples, type = "l", main = expression("Trace Plot of " * theta), xlab = "Iteration", ylab = expression(theta))

```



```{r}
png("trace_global_theta_CAD_Beta(4,4).png", width = 800, height = 600)
plot(GLOBAL_THETA, type = "l",
     main = expression("Trace Plot of Global " * theta),
     xlab = "Iteration", ylab = expression(theta))
dev.off()

```
```{r}
png("density_global_theta_CAD_Beta(4,4).png", width = 800, height = 600)
plot(density(GLOBAL_THETA),
     main = expression("Density of Global " * theta),
     xlab = expression(theta), ylab = "Density")
dev.off()

```

```{r}
effectiveSize(GLOBAL_THETA)
acf(GLOBAL_THETA)
png("acf_global_theta_CAD_Beta(4,4).png", width = 800, height = 600)
acf(GLOBAL_THETA, main = "ACF of GLOBAL_THETA")
dev.off()
```


```{r}
# ─────────────────────────────────────────────
# Credible Intervals & Posterior Selection Probabilities (PSP)
# Two exports:
#   1) Paper (black/gray)      → "CI_PSP_Paper_flexible.pdf"
#   2) Presentation (red/blue) → "CI_PSP_Presentation_flexible.pdf"
# Notes:
#   - Intercept is EXCLUDED from CI and PSP plots.
#   - Order: HIGH → LOW PSP with highest at the TOP.
#   - Side-by-side layout uses 65% (CI) and 35% (PSP).
#   - We use the term PSP everywhere (no PIP).
# ─────────────────────────────────────────────

library(ggplot2)
library(gridExtra)


# Helper to compute 2.5%, 50%, 97.5% quantiles per column
calculate_quantiles <- function(mat, probs = c(0.025, 0.5, 0.975)) {
  apply(mat, 2, quantile, probs = probs)
}

# Effective coefficients and summaries
BETA_ZLV_thinned <- BETA_thinned * ZLV_thinned
beta_q  <- calculate_quantiles(BETA_ZLV_thinned)
beta_df <- as.data.frame(t(beta_q))
beta_df$Variable <- rownames(beta_df)
colnames(beta_df)[1:3] <- c("Lower","Median","Upper")

# PSP (Posterior Selection Probability)
psp     <- colMeans(ZLV_thinned)
psp_df  <- data.frame(Variable = names(psp), PSP = as.numeric(psp), row.names = NULL)

# Merge, mark significance by 95% CI, drop intercept
combined <- merge(beta_df, psp_df, by = "Variable", all.x = TRUE)
#combined$Significance <- ifelse(combined$Lower < 0 & combined$Upper > 0, "Not significant", "Significant")
combined$Significance <- ifelse(combined$Lower <= 0 & combined$Upper >= 0, "Not significant", "Significant")

combined <- combined[combined$Variable != "(Intercept)", , drop = FALSE]

# Order from HIGH to LOW PSP and make labels
combined <- combined[order(-combined$PSP), ]
combined$Label <- combined$Variable

# IMPORTANT: reverse factor levels so the highest PSP appears at the TOP after coord_flip()
combined$Label <- factor(combined$Label, levels = rev(combined$Label))

# ── STYLE 1: PAPER (black/gray) ──────────────────────────────
p_ci_paper <- ggplot(combined, aes(x = Label, y = Median)) +
  geom_point(size = 2.8, aes(color = Significance)) +
  geom_errorbar(aes(ymin = Lower, ymax = Upper, color = Significance), width = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  scale_color_manual(values = c("Significant" = "black", "Not significant" = "gray")) +
  coord_flip() +
  labs(title = "Credible intervals", x = NULL, y = NULL) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none")

p_psp_paper <- ggplot(combined, aes(x = Label, y = PSP, fill = Significance)) +
  geom_bar(stat = "identity", width = 0.8) +
  scale_fill_manual(values = c("Significant" = "black", "Not significant" = "gray")) +
  coord_flip() +
  labs(title = "Posterior selection probability", x = NULL, y = NULL) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none",
        axis.text.y = element_blank())

pdf("CI_PSP_Paper_flexible_CAD_Beta(4,4).pdf", width = 12, height = 7)
grid.arrange(p_ci_paper, p_psp_paper, ncol = 2, widths = c(0.65, 0.35))
dev.off()

# ── STYLE 2: PRESENTATION (red dots + blue intervals/bars) ──
p_ci_pres <- ggplot(combined, aes(x = Label, y = Median)) +
  geom_point(size = 3.2, aes(color = Significance)) +                           # red/gray dots
  geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.25, color = "steelblue") +  # blue CI
  geom_hline(yintercept = 0, linetype = "dashed") +
  scale_color_manual(values = c("Significant" = "red", "Not significant" = "gray")) +
  coord_flip() +
  labs(title = "Credible intervals", x = NULL, y = NULL) +
  theme_minimal(base_size = 13) +
  theme(legend.position = "none")

p_psp_pres <- ggplot(combined, aes(x = Label, y = PSP)) +
  geom_bar(stat = "identity", width = 0.8, fill = "steelblue") +
  coord_flip() +
  labs(title = "Posterior selection probability", x = NULL, y = NULL) +
  theme_minimal(base_size = 13) +
  theme(axis.text.y = element_blank())

pdf("CI_PSP_Presentation_flexible_CAD_Beta(4,4).pdf", width = 12, height = 7)
grid.arrange(p_ci_pres, p_psp_pres, ncol = 2, widths = c(0.65, 0.35))
dev.off()

```




```{r}
# ─────────────────────────────────────────────────────────────
# Odds Ratios + PSP Tables (CSV incl. intercept; LaTeX w/o intercept)
# Exports:
#   • CSV (includes intercept):            OR_PSP_AllPredictors_flexible.csv
#   • LaTeX (paper, no Width cols):        OR_PSP_Table_NoIntercept_PAPER_flexible.tex
#   • LaTeX (presentation, with Width & %):OR_PSP_Table_NoIntercept_PRESENT_flexible.tex
# Notes:
#   • ORs = exp of the 2.5%, 50%, 97.5% quantiles of β·Z (post-thinning).
#   • PSP = Posterior Selection Probability = mean(Z).
#   • LaTeX tables EXCLUDE intercept (as requested).
#   • Rows ordered by PSP (high → low).
# ─────────────────────────────────────────────────────────────



# 2) Posterior quantiles of β·Z (log-odds)
beta_q <- t(apply(BETA_ZLV_thinned, 2, quantile, probs = c(0.025, 0.5, 0.975)))
beta_df <- data.frame(
  Variable = rownames(beta_q),
  Lower = beta_q[, 1],
  Median = beta_q[, 2],
  Upper = beta_q[, 3],
  row.names = NULL
)

# 3) PSP (Posterior Selection Probability)
psp <- colMeans(ZLV_thinned)
psp_df <- data.frame(Variable = names(psp), PSP = as.numeric(psp))

# 4) Merge and compute ORs (exp) and width metrics
combined <- merge(beta_df, psp_df, by = "Variable", all.x = TRUE)
combined$Label     <- combined$Variable
combined$OR_2.5    <- exp(combined$Lower)
combined$OR_50     <- exp(combined$Median)
combined$OR_97.5   <- exp(combined$Upper)
combined$Width     <- combined$OR_97.5 - combined$OR_2.5                          # absolute width (OR-scale)
combined$WidthPct  <- ifelse(combined$OR_50 == 0, NA_real_,
                             100 * (combined$OR_97.5 - combined$OR_2.5) / combined$OR_50) # % shrinkage relative to OR_50

# 5) Order by PSP (high → low)
combined <- combined[order(-combined$PSP), ]

# 6) CSV (INCLUDES INTERCEPT) for full record
csv_out <- combined[, c("Variable", "Label", "OR_50", "OR_2.5", "OR_97.5", "Width", "WidthPct", "PSP")]
write.csv(csv_out, "OR_PSP_AllPredictors_flexible_beta_bc_(1,1).csv", row.names = FALSE)

# 7) LaTeX TABLES (EXCLUDE INTERCEPT)
latex_df <- subset(combined, Variable != "(Intercept)")

# Rounding for display
latex_fmt <- within(latex_df, {
  OR_2.5   <- round(OR_2.5,  3)
  OR_50    <- round(OR_50,   3)
  OR_97.5  <- round(OR_97.5, 3)
  Width    <- round(Width,   3)
  PSP      <- round(PSP,     3)
  WidthPct <- round(WidthPct, 1)  # one decimal place in percent
})

# ── PAPER VERSION (no Width columns) ─────────────────────────
latex_paper <- latex_fmt[, c("Label", "OR_50", "OR_2.5", "OR_97.5", "PSP")]
colnames(latex_paper) <- c("Predictor", "Odds Ratio", "2.5\\% CI", "97.5\\% CI", "PSP")

latex_code_paper <- knitr::kable(
  latex_paper,
  format  = "latex",
  booktabs= TRUE,
  align   = c("l","c","c","c","c"),
  caption = "Odds ratios with 95\\% credible intervals and Posterior Selection Probability (PSP). Intercept excluded."
)

writeLines(latex_code_paper, con = "OR_PSP_Table_NoIntercept_PAPER_flexible_beta_bc_(1,1).tex")
cat(latex_code_paper)  # also print to console

# ── PRESENTATION VERSION (adds Width + % Shrinkage) ─────────
latex_present <- latex_fmt[, c("Label", "OR_50", "OR_2.5", "OR_97.5", "Width", "WidthPct", "PSP")]
# format percentage with % sign for LaTeX
latex_present$WidthPct <- ifelse(is.na(latex_present$WidthPct),
                                 "--",
                                 paste0(latex_present$WidthPct, "\\%"))

colnames(latex_present) <- c("Predictor", "Odds Ratio", "2.5\\% CI", "97.5\\% CI", "Width", "Width Shrinkage", "PSP")

latex_code_present <- knitr::kable(
  latex_present,
  format  = "latex",
  booktabs= TRUE,
  align   = c("l","c","c","c","c","c","c"),
  caption = "Odds ratios with 95\\% credible intervals, Width, Width Shrinkage, and Posterior Selection Probability (PSP). Intercept excluded."
)

writeLines(latex_code_present, con = "OR_PSP_Table_NoIntercept_PRESENT_flexible_beta_bc_(1,1).tex")
cat(latex_code_present)  # also print to console

```




```{r}
# 4) Merge and width metrics on LOG-ODDS scale
combined <- merge(beta_df, psp_df, by = "Variable", all.x = TRUE)
combined$Label    <- combined$Variable
combined$Width    <- combined$Upper - combined$Lower
combined$WidthPct <- ifelse(abs(combined$Median) < .Machine$double.eps, 
                            NA_real_,
                            100 * (combined$Upper - combined$Lower) / abs(combined$Median))

# Add Significance: TRUE if 95% CI excludes 0
combined$Significant <- with(combined, ifelse(Lower > 0 | Upper < 0, TRUE, FALSE))

# 5) Order by PSP (high → low)
combined <- combined[order(-combined$PSP), ]

# 6) CSV (INCLUDES INTERCEPT) for full record
csv_out <- combined[, c("Variable", "Label", "Median", "Lower", "Upper", 
                        "Width", "WidthPct", "PSP", "Significant")]
write.csv(csv_out, "OR_PSP_AllPredictors_flexible_CAD_Beta(4,4).csv", row.names = FALSE)

```



```{r}
# Evaluating the Model on Test Data 
predict_proba <- function(X, beta, zlv) {
  plogis(X %*% (beta * zlv))
}

beta_mean <- colMeans(BETA_thinned)
zlv_mean <- colMeans(ZLV_thinned)

train_preds <- predict_proba(X_train, beta_mean, zlv_mean)
#valid_preds <- predict_proba(X_valid, beta_mean, zlv_mean)
test_preds <- predict_proba(X_test, beta_mean, zlv_mean)

train_preds_binary <- ifelse(train_preds > 0.5, 1, 0)
#valid_preds_binary <- ifelse(valid_preds > 0.5, 1, 0)
test_preds_binary <- ifelse(test_preds > 0.5, 1, 0)

# Calculating Performance Metrics
calculate_metrics <- function(true_labels, pred_probs, pred_labels) {
  roc_curve <- roc(true_labels, pred_probs)
  auc <- auc(roc_curve)
  confusion <- confusionMatrix(factor(pred_labels), factor(true_labels))
  
  accuracy <- confusion$overall['Accuracy']
  f1 <- confusion$byClass['F1']
  sensitivity <- confusion$byClass['Sensitivity']
  specificity <- confusion$byClass['Specificity']
  precision <- confusion$byClass['Precision']
  error_rate <- 1 - accuracy
  fpr <- 1 - specificity
  fnr <- 1 - sensitivity
  tnr <- specificity
  
  list(
    Accuracy = accuracy,
    F1_Score = f1,
    Sensitivity = sensitivity,
    Specificity = specificity,
    Precision = precision,
    Error_Rate = error_rate,
    FPR = fpr,
    FNR = fnr,
    TNR = tnr,
    AUC = auc
  )
}

train_metrics <- calculate_metrics(y_train, train_preds, train_preds_binary)
#valid_metrics <- calculate_metrics(y_valid, valid_preds, valid_preds_binary)
test_metrics <- calculate_metrics(y_test, test_preds, test_preds_binary)

metrics_df <- data.frame(
  Metric = names(train_metrics),
  Train = unlist(train_metrics),
  #Validation = unlist(valid_metrics),
  Test = unlist(test_metrics)
)

pdf("performance_metrics_comparison_Beta(4,4).pdf", width = 8, height = 11)
grid.table(metrics_df)
dev.off()

```







```{r}
# Step 1: Compute log-likelihood at each MCMC iteration
loglik_samples <- numeric(nrow(BETA_thinned))
for (i in 1:nrow(BETA_thinned)) {
  loglik_samples[i] <- flogis.log(BETA_thinned[i, ], ZLV_thinned[i, ], X_train, y_train)
}
deviance_samples <- -2 * loglik_samples

# Step 2: Posterior mean deviance
D_bar <- mean(deviance_samples)

# Step 3: Compute deviance at posterior mean of (beta * Z)
# Note: this is the key correction — combining them first, then averaging
beta_zlv_mean <- colMeans(BETA_thinned * ZLV_thinned)

# Step 4: Evaluate deviance at this effective mean parameter vector
# Set Z vector to 1 because you've already zeroed out coefficients
D_hat <- -2 * flogis.log(beta_zlv_mean, rep(1, length(beta_zlv_mean)), X_train, y_train)

# Step 5: Effective number of parameters
p_D <- D_bar - D_hat

# Step 6: Compute DIC
DIC <- D_bar + p_D

# Step 7: Print result
cat("Posterior mean deviance (D̄):", round(D_bar, 2), "\n")
cat("Deviance at mean (D̂):", round(D_hat, 2), "\n")
cat("Effective number of parameters (pD):", round(p_D, 2), "\n")
cat("Deviance Information Criterion (DIC):", round(DIC, 2), "\n")

```





```{r}
# Posterior probability that each parameter > 0
ppd_positive <- colMeans(BETA_ZLV_thinned > 0)

# Posterior probability that each parameter < 0
ppd_negative <- colMeans(BETA_ZLV_thinned < 0)

# Combine into a data frame
ppd_df <- data.frame(
  Variable = colnames(BETA_ZLV_thinned),
  PPD_Positive = ppd_positive,
  PPD_Negative = ppd_negative
)

```






