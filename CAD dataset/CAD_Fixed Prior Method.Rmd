---
title: "OVERALL-Fixed Prior"
author: "Ayisha - TRU- T00727585"
date: "2024-06-27"
output: html_document
---





```{r}
# Loading the Libraries
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
library(ISLR)
library(dplyr)
library(tidyr)
library(coda)
library(reshape)
library(stats4)
library(MCMCpack)
library(ggplot2)
library(MASS)
library(Matrix)
library(knitr)
library(readr)
library(mvtnorm)
library(stargazer)
library(pROC)
library(caret)
library(gridExtra)
library(grid)
```


```{r}
# Reading the Data
trainData <- read.csv("training_data_tt.csv", header = TRUE, stringsAsFactors = FALSE)
testData <- read.csv("test_data_tt.csv", header = TRUE, stringsAsFactors = FALSE)

# Defining the Logistic Function
flogis.log <- function(beta, zlv, X, y) {
  p <- plogis(X %*% (beta * zlv))
  sum(ifelse(y, log(p), log(1 - p)))
}

# Preparing the Data for Analysis
y_train <- trainData[, "HeartDisease"]
X_train <- model.matrix(as.formula(paste("HeartDisease ~ .")), data = trainData)
y_test <- testData[, "HeartDisease"]
X_test <- model.matrix(as.formula(paste("HeartDisease ~ .")), data = testData)
k <- ncol(X_train)
n <- nrow(X_train)
```


```{r}
# Step 1: Performing Frequentist Logistic Regression for Initial Beta Estimates
reg.glm <- glm(HeartDisease ~ ., data = trainData, family = binomial)
BETA_init <- as.vector(reg.glm$coef)
vcov_matrix <- vcov(reg.glm)


# Step 2: Calculating the Prior Mean and Standard Deviation for Beta
pmn.beta <- BETA_init
psd.beta <- vcov_matrix

# Initiating MCMC Sampling
S <- 140000
beta <- reg.glm$coef
zlv <- rep(1, k)
acsb <- acsg <- 0
BETA <- matrix(0, nrow = S, ncol = k)
ZLV <- matrix(0, nrow = S, ncol = k)
accept_beta <- rep(0, k)
accept_zlv <- rep(0, k - 1)
short_chain_length<-1000
# Setting Proposal Variance for MCMC
c <- 1

```


```{r}
# Track acceptance rate
set.seed(1)  # For reproducibility
var.prop <- c * vcov_matrix
for (i in 1:10) {  # Adjust the proposal variance over 10 iterations of tuning
  acs <- 0  # Acceptance count
  
  for (s in 1:short_chain_length) {
    # Propose a new beta using current variance scale (c * vcov_matrix)
    beta.p <- t(rmvnorm(1, beta, c * vcov_matrix))
    
    # Calculate log acceptance ratio (log-hastings ratio)
    lhr <- flogis.log(beta.p, zlv, X_train, y_train) - flogis.log(beta, zlv, X_train, y_train) +
           dmvnorm(t(beta.p), BETA_init, vcov_matrix, log = TRUE) -
           dmvnorm(t(beta), BETA_init, vcov_matrix, log = TRUE)
    
    # Accept or reject based on acceptance ratio
    if (log(runif(1)) < lhr) {
      beta <- beta.p
      acs <- acs + 1
    }
  }
  
  # Calculate acceptance rate
  acceptance_rate <- acs / short_chain_length
  print(paste("Iteration", i, "- Acceptance rate:", acceptance_rate))
  
  # Adjust `c` based on acceptance rate
  if (acceptance_rate < 0.2) {
    c <- c / 2  # Reduce `c` if acceptance rate is too low
  } else if (acceptance_rate > 0.5) {
    c <- c * 2  # Increase `c` if acceptance rate is too high
  }
  
  # Print adjusted `c` value for tracking
  print(paste("Adjusted proposal variance scaling factor (c):", c))
}

# After tuning, `c` can be used in the main MCMC chain
print(paste("Final proposal variance scaling factor (c):", c))
```





```{r}
# Running Full MCMC Sampling
set.seed(1)

start_time <- Sys.time()
var.prop <- c * vcov_matrix
for (s in 1:S) {
  beta.p <- t(rmvnorm(1, beta, var.prop))
  
  lhr <- flogis.log(beta.p, zlv, X_train, y_train) - flogis.log(beta, zlv, X_train, y_train) +
         dmvnorm(t(beta.p), pmn.beta, psd.beta, log = TRUE) -
         dmvnorm(t(beta), pmn.beta, psd.beta, log = TRUE)
  
  if (log(runif(1)) < lhr) {
    accept_beta <- accept_beta + (beta.p != beta)
    beta <- beta.p
    acsb <- acsb + 1
  }
  
  BETA[s, ] <- beta
  
  for (j in 1:k) {
    zlv.p <- zlv
    zlv.p[j] <- 1 - zlv.p[j]
    
    lhg <- flogis.log(pmn.beta, zlv.p, X_train, y_train) - flogis.log(pmn.beta, zlv, X_train, y_train) +
           sum(dbinom(zlv.p[c(1:k)], size = 1, prob = 0.5, log = TRUE)) - 
           sum(dbinom(zlv[c(1:k)], size = 1, prob = 0.5, log = TRUE))
    
    if (log(runif(1)) < lhg) {
      zlv[j] <- zlv.p[j]
      acsg <- acsg + 1
      accept_zlv[j-1] <- accept_zlv[j-1] + 1
    }
  }
  
  ZLV[s, ] <- zlv
}

end_time <- Sys.time()


#colnames(X_train)
```

```{r}
# Save the results
save(BETA, ZLV, file = "OAM1_results_FIMS.RData")

# To load the results back in future sessions
load("OAM1_results_FIMS.RData")
```


```{r}
# Rename selected variables
colnames(X_train)[colnames(X_train) == "MHR"] <- "Max HR"
colnames(X_train)[colnames(X_train) == "RBPS"] <- "Resting BP"
colnames(X_train)[colnames(X_train) == "Chol"] <- "Cholesterol"
colnames(X_train)[colnames(X_train) == "AgeGrp"] <- "Age group"
```

```{r}
# Define variable names 
variable_names <- colnames(X_train)
print(paste("Variable Names:", variable_names))

# Calculating Acceptance Rates
acceptance_rate_beta <- acsb / S
acceptance_rate_zlv <- acsg / (S * (k - 1))

print(paste("Acceptance rate for beta:", acceptance_rate_beta))
print(paste("Acceptance rate for zlv:", acceptance_rate_zlv))

# Ensure we have names for each column in BETA and ZLV
colnames(BETA) <- variable_names
colnames(ZLV) <- variable_names
BETA_ZLV<-BETA *ZLV
colnames(BETA_ZLV) <- variable_names
```


```{r}
# 1. Effective Sample Size (ESS) for `BETA` and `ZLV`
ess_beta <- effectiveSize(as.mcmc(BETA))
ess_zlv <- effectiveSize(as.mcmc(ZLV))
ess_beta_zlv <- effectiveSize(as.mcmc(ZLV))

# Display ESS for both in a summary table
ess_summary <- data.frame(
  Parameter = variable_names,
  ESS_Beta = round(ess_beta, 2),
  ESS_ZLV = round(ess_zlv, 2),
  ESS_BETA_ZLV= round(ess_beta_zlv, 2)
)
print(ess_summary)
# Save ESS Summary as a PDF
pdf("ESS_Summary_Beta_ZLV_OA1.pdf", width = 8, height = 5)
grid.table(ess_summary)
dev.off()
```




```{r}
# Plotting Functions for Trace and ACF Plots
plot_trace_density_to_pdf <- function(param_matrix, param_names, file_name) {
  pdf(file_name, width = 8, height = 11)
  ncol_param <- ncol(param_matrix)
  par(mfrow = c(ceiling(ncol_param / 3), 2), mar = c(5, 5, 4, 2))
  
  for (i in 1:ncol_param) {
    plot(param_matrix[, i], type = 'l', 
         main = paste("Traceplot for", param_names[i]),
         xlab = "Iteration", 
         ylab = "Value")
    plot(density(param_matrix[, i]), 
         main = paste("Density plot for", param_names[i]),
         xlab = "Value", 
         ylab = "Density")
  }
  dev.off()
}

plot_acf_to_pdf <- function(param_matrix, param_names, file_name) {
  pdf(file_name, width = 8, height = 11)
  ncol_param <- ncol(param_matrix)
  par(mfrow = c(ceiling(ncol_param / 3), 2), mar = c(5, 5, 4, 2))
  
  for (i in 1:ncol_param) {
    acf(param_matrix[, i], main = paste("ACF for", param_names[i]), xlab = "Lag", ylab = "ACF")
  }
  dev.off()
}


# Trace Plots Before Thinning
plot_trace_density_to_pdf(BETA, variable_names, "Beta_Trace_Density_Before_Thinning_OAM1_FIMS.pdf")
plot_trace_density_to_pdf(BETA * ZLV, variable_names, "BG_Trace_Density_Before_Thinning_OAM1_FIMS.pdf")
plot_acf_to_pdf(BETA, variable_names, "Beta_ACF_Before_Thinning_OAM1_FIMS.pdf")
plot_acf_to_pdf(BETA * ZLV, variable_names, "BG_ACF_Before_Thinning_OAM1_FIMS.pdf")
```


```{r}

# Removing Initial Samples and Applying Thinning
BETA <- BETA[-c(1:20000), ]
ZLV <- ZLV[-c(1:20000), ]

thin_interval <- 60
BETA_thinned <- BETA[seq(1, nrow(BETA), by = thin_interval), ]
ZLV_thinned <- ZLV[seq(1, nrow(ZLV), by = thin_interval), ]

# Combining BETA and ZLV for Plotting
BETA_ZLV_thinned <- BETA_thinned * ZLV_thinned

# Trace and ACF Plots After Thinning
plot_trace_density_to_pdf(BETA_thinned, variable_names, "Beta_Trace_Density_Thinned_OAM1.pdf")
plot_trace_density_to_pdf(ZLV_thinned, variable_names, "ZLV_Trace_Density_Thinned_OAM1.pdf")
plot_trace_density_to_pdf(BETA_ZLV_thinned, variable_names, "BG_Trace_Density_Thinned_OAM1.pdf")
plot_acf_to_pdf(BETA_thinned, variable_names, "Beta_ACF_Thinned_OAM1_FIMS.pdf")
plot_acf_to_pdf(BETA_ZLV_thinned, variable_names, "BG_ACF_Thinned_OAM1_FIMS.pdf")
```


```{r}
# Calculating the Top 5 Models
ZLV_list <- as.list(as.data.frame(ZLV))
top5_models <- do.call(paste0, ZLV_list) %>% table() %>% sort(decreasing = TRUE) %>% .[1:5]
top5_probabilities <- top5_models / sum(top5_models)

top5_df <- data.frame(
  Model = paste0("Model ", 1:5),
  Probability = round(as.numeric(top5_probabilities), 2),
  Variables = names(top5_probabilities)
)

# Plotting Posterior Probabilities of the Top 5 Models
plot <- ggplot(top5_df, aes(x = Model, y = Probability)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Probability, 2)), vjust = -0.5) +
  labs(title = "Posterior Probabilities of Top 5 Models- Overall- Method1", x = "Model", y = "Posterior Probability") +
  theme_minimal()

ggsave("posterior_probabilities_top5_models_OAM1.pdf", plot = plot, width = 10, height = 6)

# Creating Table for the Top 5 Models
table_grob <- tableGrob(top5_df)
pdf("top5_models_table_OAM1_FIMS.pdf", width = 8, height = 6)
grid.draw(table_grob)
dev.off()
```

```{r}
# Load Required Libraries
library(ggplot2)
library(gridExtra)

# --- Helper: Calculate Credible Intervals ---
calculate_quantiles <- function(beta_matrix, probs = c(0.025, 0.5, 0.975)) {
  apply(beta_matrix, 2, quantile, probs = probs)
}

# --- Step 1: Prepare Credible Interval Data ---
beta_quantiles <- calculate_quantiles(BETA_ZLV_thinned)
variable_names <- colnames(BETA_ZLV_thinned)
beta_means <- colMeans(BETA_ZLV_thinned)

# Create dataframe
beta_df <- as.data.frame(t(beta_quantiles))
beta_df$Variable <- variable_names
beta_df <- beta_df[, c("Variable", "2.5%", "50%", "97.5%")]

# --- Step 2: Identify Significant Predictors ---
significant_predictors <- variable_names[apply(beta_quantiles, 2, function(x) !(x[1] < 0 & x[3] > 0))]
beta_df$Significance <- ifelse(beta_df$Variable %in% significant_predictors, "Significant", "Not-significant")

# --- Step 3: Compute Posterior selection  Probabilities (PIP) ---
pip <- colMeans(ZLV_thinned)
pip_df <- data.frame(Variable = variable_names, PIP = pip)
pip_df$Significance <- ifelse(pip_df$Variable %in% significant_predictors, "Significant", "Not-significant")

# --- Step 4: Combine into Single DataFrame ---
combined_df <- merge(beta_df, pip_df, by = "Variable", suffixes = c("_CI", "_PIP"))
combined_df$Width <- combined_df$`97.5%` - combined_df$`2.5%`
combined_df <- combined_df[, c("Variable", "2.5%", "50%", "97.5%", "Width", "Significance_CI", "PIP", "Significance_PIP")]

# --- Step 5: Remove Intercept ---
combined_df <- combined_df[combined_df$Variable != "(Intercept)", ]
combined_df$Variable <- factor(combined_df$Variable, levels = combined_df$Variable[order(combined_df$PIP)])

# --- Step 6: PIP Plot (no legend, gray fill) ---
p2 <- ggplot(combined_df, aes(x = Variable, y = PIP, fill = Significance_PIP)) +
  geom_bar(stat = "identity", width = 0.8) +
  scale_fill_manual(values = c("Significant" = "black", "Not-significant" = "gray")) +
  coord_flip() +
  labs(title = "Posterior selection  Probability", x = NULL, y = NULL) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.text.y = element_blank(),
    axis.text.x = element_text(size = 8),
    legend.position = "none"
  )

# --- Step 7: Credible Interval Plot (gray points + error bars) ---
combined_df$Variable <- factor(combined_df$Variable, levels = levels(combined_df$Variable))  # keep order

p1 <- ggplot(combined_df, aes(x = Variable, y = `50%`, color = Significance_CI)) +
  geom_point(size = 3, color = "darkgray") +
  geom_errorbar(aes(ymin = `2.5%`, ymax = `97.5%`), width = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  scale_color_manual(values = c("Significant" = "black", "Not-significant" = "gray")) +
  labs(title = "Credible Interval", x = NULL, y = NULL) +
  coord_flip() +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.text.y = element_text(size = 8, hjust = 1),
    axis.text.x = element_text(size = 8),
    legend.position = "none"
  )

# --- Step 8: Save Plots ---
ggsave("Credible_interval_plot_OA1.pdf", plot = p1, width = 8, height = 6)
pdf("Credible_Interval_and_PIP_Plots_OA1.pdf", width = 12, height = 8)
grid.arrange(p1, p2, ncol = 2)
dev.off()

pdf("Credible_Interval_and_PIP_Plots_WidthOA1.pdf", width = 12, height = 8)
grid.arrange(p1, p2, ncol = 2, widths = c(0.65, 0.35))
dev.off()

# --- Step 9: Save Summary Table ---
write.csv(combined_df, "Predictor_Significance_and_PIP_Fixed.csv", row.names = FALSE)
pdf("Significance_and_PIP_Table_Fixed.pdf", width = 12, height = 8)
grid.table(combined_df)
dev.off()

# --- Optional: Print objects ---
print(p1)
print(p2)
head(combined_df)

```







```{r}


# Define Helper Function for Credible Intervals
calculate_quantiles <- function(beta_matrix, probs = c(0.025, 0.5, 0.975)) {
  apply(beta_matrix, 2, quantile, probs = probs)
}

# Calculate Credible Intervals from BETA_ZLV_thinned
beta_quantiles <- calculate_quantiles(BETA_ZLV_thinned)
beta_means <- colMeans(BETA_ZLV_thinned)
variable_names <- colnames(BETA_ZLV_thinned)  # Extract variable names dynamically

# Prepare Data for Plotting Credible Intervals
beta_df <- as.data.frame(t(beta_quantiles))
beta_df$Variable <- variable_names
beta_df <- beta_df[, c("Variable", "2.5%", "50%", "97.5%")]

# Identify Significant Predictors Based on Credible Intervals
significant_predictors <- variable_names[apply(beta_quantiles, 2, function(x) !(x[1] < 0 & x[3] > 0))]
beta_df$Significance <- ifelse(beta_df$Variable %in% significant_predictors, "Significant", "Not Significant")

# Exclude "Intercept" ONLY for Plotting
beta_df <- beta_df[beta_df$Variable != "(Intercept)", ]

p1 <- ggplot(beta_df, aes(x = reorder(Variable, `50%`), y = `50%`, color = Significance)) +
 geom_point(size = 3, color = "red") +
  geom_errorbar(aes(ymin = `2.5%`, ymax = `97.5%`), width = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  labs(
    title = "Credible Interval- Fixed Prior- Overall",
    x = "Predictors",
    y = "Posterior Mean"
  ) +
  scale_color_manual(
    values = c("Significant" = "black", "Not Significant" = "gray"),
    name = "Significance"
  ) +
  coord_flip() +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 14, face = "bold", color = "black"), # Larger, bolder text for y-axis labels
    axis.text.x = element_text(size = 12), # Adjust x-axis text size if needed
    legend.title = element_text(size = 12, face = "bold"),
    legend.text = element_text(size = 10)
  )
ggsave("credible_interval_plot_OA1.pdf", plot = p1, width = 8, height = 6)

# Calculate Posterior selection  Probabilities (PIP) from ZLV_thinned
pip <- colMeans(ZLV_thinned)
pip_df <- data.frame(Variable = variable_names, PIP = pip)
pip_df$Significance <- ifelse(pip_df$Variable %in% significant_predictors, "Significant", "Not Significant")

# Exclude "Intercept" from PIP Data
pip_df <- pip_df[pip_df$Variable != "(Intercept)", ]

# Plot PIP Values
p2 <- ggplot(pip_df, aes(x = reorder(Variable, PIP), y = PIP, fill = Significance)) +
  geom_bar(stat = "identity", width = 0.8) +
  scale_fill_manual(
    values = c("Significant" = "black", "Not Significant" = "gray"),
    name = "Significance"
  ) +
  coord_flip() +
  labs(
    title = "PIP- Fixed Prior- Overall",
    x = "Predictors",
    y = "PIP"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 14, face = "bold", color = "black"), # Larger, bolder text for y-axis labels
    axis.text.x = element_text(size = 12), # Adjust x-axis text size if needed
    legend.title = element_text(size = 12, face = "bold"),
    legend.text = element_text(size = 10)
  )

# Save Combined Plots as Side-by-Side
pdf("Credible_Interval_and_PIP_Plots_OA1.pdf", width = 12, height = 8)
grid.arrange(p1, p2, ncol = 2)
dev.off()
pdf("Credible_Interval_and_PIP_Plots_WidthOA1.pdf", width = 12, height = 8)
grid.arrange(p1, p2, ncol = 2, widths = c(0.65, 0.35))
dev.off()
# Print Plots
print(p1)
print(p2)


```


```{r,eval=FALSE}

pip <- colMeans(ZLV_thinned)
pip_df <- data.frame(Variable = variable_names, PIP = pip)
pip_df$Significance <- ifelse(pip_df$Variable %in% significant_predictors, "Significant", "Not Significant")

# Exclude "Intercept" from both beta_df and pip_df
beta_df <- beta_df[beta_df$Variable != "(Intercept)", ]
pip_df <- pip_df[pip_df$Variable != "(Intercept)", ]

# Create the PIP plot first to ensure the order is correct
p2 <- ggplot(pip_df, aes(x = reorder(Variable, PIP), y = PIP, fill = Significance)) +
  geom_bar(stat = "identity", width = 0.8) +
  scale_fill_manual(
    values = c("Significant" = "black", "Not Significant" = "gray"),
    name = "Significance"
  ) +
  coord_flip() +
  labs(
    title = "Posterior selection  probability",
    x = " ",
    y = " "
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12, face = "bold"),
     axis.text.y = element_blank(),
    #axis.text.y = element_text(size = 14, face = "bold", color = "black"), # Larger, bolder text for y-axis labels
    axis.text.x = element_text(size = 12), # Adjust x-axis text size if needed
    legend.position = "none" # Remove legend
  )

# Extract the order of variables from the PIP plot
pip_order <- levels(reorder(pip_df$Variable, pip_df$PIP))

# Reorder the Variable column in beta_df to match the order in the PIP plot
beta_df$Variable <- factor(beta_df$Variable, levels = pip_order)

# Plot Credible Intervals with variables ordered by decreasing PIP values
p1 <- ggplot(beta_df, aes(x = Variable, y = `50%`, color = Significance)) +
  geom_point(size = 3, color = "darkgray") +
  geom_errorbar(aes(ymin = `2.5%`, ymax = `97.5%`), width = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  labs(
    title = "Credible Interval",
    x = " ",
    y = " "
  ) +
  scale_color_manual(
    values = c("Significant" = "black", "Not Significant" = "gray"),
    name = "Significance"
  ) +
  coord_flip() +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 14, face = "bold", color = "black"), # Larger, bolder text for y-axis labels
    axis.text.x = element_text(size = 12), # Adjust x-axis text size if needed
    legend.position = "none" # Remove legend
  )

# Save the Credible Interval plot
ggsave("Credible_interval_plot_OA1.pdf", plot = p1, width = 8, height = 6)

# Save Combined Plots as Side-by-Side
pdf("Credible_Interval_and_PIP_Plots_OA1.pdf", width = 12, height = 8)
grid.arrange(p1, p2, ncol = 2)
dev.off()
pdf("Credible_Interval_and_PIP_Plots_widthOA1.pdf", width = 12, height = 8)
grid.arrange(p1, p2, ncol = 2, widths = c(0.65, 0.35))
dev.off()
# Print Plots
print(p1)
print(p2)
```


```{r}

# Calculate Posterior selection  Probabilities (PIP) from ZLV_thinned
pip <- colMeans(ZLV_thinned)
pip_df <- data.frame(Variable = variable_names, PIP = pip)
pip_df$Significance <- ifelse(pip_df$Variable %in% significant_predictors, "Significant", "Not Significant")
# Exclude "Intercept" from both beta_df and pip_df
beta_df <- beta_df[beta_df$Variable != "(Intercept)", ]
pip_df <- pip_df[pip_df$Variable != "(Intercept)", ]

# Create the PIP plot first to ensure the order is correct
p2 <- ggplot(pip_df, aes(x = reorder(Variable, PIP), y = PIP, fill = Significance)) +
  geom_bar(stat = "identity", width = 0.8) +
  scale_fill_manual(
    values = c("Significant" = "black", "Not Significant" = "gray"),
    name = "Significance"
  ) +
  coord_flip() +
  labs(
    title = "Posterior selection probablity",
    x = " ",
    y = " "
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12, face = "bold"),
    axis.text.y =element_blank(),
    #axis.text.y = element_text(size = 8, hjust = 1), # Reduced font size for y-axis
    axis.text.x = element_text(size = 8), # Reduced font size for x-axis
    legend.position = "none" # Remove legend
  )

# Extract the order of variables from the PIP plot

library(ggplot2)
pip_order <- levels(reorder(pip_df$Variable, pip_df$PIP))

# Reorder the Variable column in beta_df to match the order in the PIP plot
beta_df$Variable <- factor(beta_df$Variable, levels = pip_order)

# Plot Credible Intervals with variables ordered by decreasing PIP values
p1 <- ggplot(beta_df, aes(x = Variable, y = `50%`, color = Significance)) +
  geom_point(size = 3, color = "darkgray") +
  geom_errorbar(aes(ymin = `2.5%`, ymax = `97.5%`), width = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  labs(
    title = "Credible interval",
    x = " ",
    y = " "
  ) +
  scale_color_manual(
    values = c("Significant" = "black", "Not Significant" = "gray"),
    name = "Significance"
  ) +
  coord_flip() +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 14, face = "bold", color = "black"), # Larger, bolder text for y-axis labels
    axis.text.x = element_text(size = 12), # Adjust x-axis text size if needed
    legend.position = "none" # Remove legend
  )

# Save the Credible Interval plot
ggsave("Credible_interval_plot_OA1.pdf", plot = p1, width = 8, height = 6)

# Save Combined Plots as Side-by-Side
pdf("Credible_Interval_and_PIP_Plots_OA1.pdf", width = 12, height = 8)
grid.arrange(p1, p2, ncol = 2)
dev.off()
pdf("Credible_Interval_and_PIP_Plots_widthOA1.pdf", width = 12, height = 8)
grid.arrange(p1, p2, ncol = 2, widths = c(0.65, 0.35))
dev.off()
# Print Plots
print(p1)
print(p2)
```


```{r}
significant_predictors
```


```{r}
# colmeans of beta_zlv thinned
beta_means
```



```{r}
colMeans(BETA_thinned) 

```


```{r}
# Load Required Libraries
library(ggplot2)
library(gridExtra)

# Define Helper Function for Credible Intervals
calculate_quantiles <- function(beta_matrix, probs = c(0.025, 0.5, 0.975)) {
  apply(beta_matrix, 2, quantile, probs = probs)
}

# Calculate Credible Intervals from BETA_ZLV_thinned
beta_quantiles <- calculate_quantiles(BETA_ZLV_thinned)
beta_means <- colMeans(BETA_ZLV_thinned)
variable_names <- colnames(BETA_ZLV_thinned)  # Extract variable names dynamically

# Prepare Data for Credible Intervals
beta_df <- as.data.frame(t(beta_quantiles))
beta_df$Variable <- variable_names
beta_df <- beta_df[, c("Variable", "2.5%", "50%", "97.5%")]

# Identify Significant Predictors Based on Credible Intervals
significant_predictors <- variable_names[apply(beta_quantiles, 2, function(x) !(x[1] < 0 & x[3] > 0))]
beta_df$Significance <- ifelse(beta_df$Variable %in% significant_predictors, "Significant", "Not Significant")

# Calculate Posterior selection  Probabilities (PIP) from ZLV_thinned
pip <- colMeans(ZLV_thinned)
pip_df <- data.frame(Variable = variable_names, PIP_Fixed = pip)
pip_df$Significance <- ifelse(pip_df$Variable %in% significant_predictors, "Significant", "Not Significant")

# Combine Credible Intervals and PIP into a Single DataFrame
combined_df <- merge(beta_df, pip_df, by = "Variable", suffixes = c("_CI", "_PIP"))

# Compute Width of the 95% Credible Interval for Fixed Prior
combined_df$Width_Fixed <- combined_df$`97.5%` - combined_df$`2.5%`

# Select relevant columns
combined_df <- combined_df[, c("Variable", "2.5%", "50%", "97.5%", "Width_Fixed", "Significance_CI", "PIP_Fixed", "Significance_PIP")]

# Exclude "Intercept" from the Combined DataFrame
combined_df <- combined_df[combined_df$Variable != "(Intercept)", ]

# Print Combined DataFrame
print(combined_df)

# Save Combined DataFrame as CSV
write.csv(combined_df, "Predictor_Significance_and_PIP_Fixed.csv", row.names = FALSE)

# Visualize DataFrame as a Table
pdf("Significance_and_PIP_Table_Fixed.pdf", width = 12, height = 8)
grid.table(combined_df)
dev.off()

```


```{r}
# Convert Log-Odds to Odds Ratios
combined_df$OR_50 <- exp(combined_df$`50%`)
combined_df$OR_Lower <- exp(combined_df$`2.5%`)
combined_df$OR_Upper <- exp(combined_df$`97.5%`)

# Reorder columns to place ORs first (optional for presentation)
combined_df <- combined_df[, c("Variable", "OR_50", "OR_Lower", "OR_Upper",
                               "2.5%", "50%", "97.5%", 
                               "Width_Fixed", 
                               "Significance_CI", "PIP_Fixed", "Significance_PIP")]

# Print Updated Table
print(combined_df)

# Save Updated Table to CSV
write.csv(combined_df, "Predictor_Significance_and_PIP_Flexible_OR.csv", row.names = FALSE)

# Optional: Save Updated Table as PDF
library(gridExtra)
pdf("Significance_and_PIP_Table_Flexible_with_OR.pdf", width = 13, height = 8)
grid.table(combined_df)
dev.off()
```











```{r}

# Counting Number of Times Each Variable is Selected as a Predictor
zlv_counts <- colSums(ZLV)
names(zlv_counts) <- variable_names
print(zlv_counts)

# Calculating the Acceptance Rate for zlv for Each Predictor
zlv_acceptance_rates <- accept_zlv / S
names(zlv_acceptance_rates) <- variable_names[-1]

acceptance_rate_zlv_full <- c(NA, zlv_acceptance_rates)
names(acceptance_rate_zlv_full) <- variable_names
print(acceptance_rate_zlv_full)

zlv_counts_df <- data.frame(
  Variable = variable_names,
  SelectionCount = zlv_counts,
  AcceptanceRate = acceptance_rate_zlv_full
)

pdf("variable_selection_counts_and_acceptance_rates_OAM1.pdf", width = 8, height = 11)
grid.table(zlv_counts_df)
dev.off()

print(paste("Total computing time OAM1:", end_time - start_time))
```

print(paste("Total computing time OAM1:", end_time - start_time))
```{r}
# Evaluating the Model on Test Data 
predict_proba <- function(X, beta, zlv) {
  plogis(X %*% (beta * zlv))
}

beta_mean <- colMeans(BETA_thinned)
zlv_mean <- colMeans(ZLV_thinned)

train_preds <- predict_proba(X_train, beta_mean, zlv_mean)
#valid_preds <- predict_proba(X_valid, beta_mean, zlv_mean)
test_preds <- predict_proba(X_test, beta_mean, zlv_mean)

train_preds_binary <- ifelse(train_preds > 0.5, 1, 0)
#valid_preds_binary <- ifelse(valid_preds > 0.5, 1, 0)
test_preds_binary <- ifelse(test_preds > 0.5, 1, 0)

# Calculating Performance Metrics
calculate_metrics <- function(true_labels, pred_probs, pred_labels) {
  roc_curve <- roc(true_labels, pred_probs)
  auc <- auc(roc_curve)
  confusion <- confusionMatrix(factor(pred_labels), factor(true_labels))
  
  accuracy <- confusion$overall['Accuracy']
  f1 <- confusion$byClass['F1']
  sensitivity <- confusion$byClass['Sensitivity']
  specificity <- confusion$byClass['Specificity']
  precision <- confusion$byClass['Precision']
  error_rate <- 1 - accuracy
  fpr <- 1 - specificity
  fnr <- 1 - sensitivity
  tnr <- specificity
  
  list(
    Accuracy = accuracy,
    F1_Score = f1,
    Sensitivity = sensitivity,
    Specificity = specificity,
    Precision = precision,
    Error_Rate = error_rate,
    FPR = fpr,
    FNR = fnr,
    TNR = tnr,
    AUC = auc
  )
}

train_metrics <- calculate_metrics(y_train, train_preds, train_preds_binary)
#valid_metrics <- calculate_metrics(y_valid, valid_preds, valid_preds_binary)
test_metrics <- calculate_metrics(y_test, test_preds, test_preds_binary)

metrics_df <- data.frame(
  Metric = names(train_metrics),
  Train = unlist(train_metrics),
  #Validation = unlist(valid_metrics),
  Test = unlist(test_metrics)
)

pdf("performance_metrics_comparison_OAM1.pdf", width = 8, height = 11)
grid.table(metrics_df)
dev.off()

```








```{r}
# Step 1: Compute log-likelihood at each MCMC iteration
loglik_samples <- numeric(nrow(BETA_thinned))
for (i in 1:nrow(BETA_thinned)) {
  loglik_samples[i] <- flogis.log(BETA_thinned[i, ], ZLV_thinned[i, ], X_train, y_train)
}
deviance_samples <- -2 * loglik_samples

# Step 2: Posterior mean deviance
D_bar <- mean(deviance_samples)

# Step 3: Compute deviance at posterior mean of (beta * Z)
# Note: this is the key correction — combining them first, then averaging
beta_zlv_mean <- colMeans(BETA_thinned * ZLV_thinned)

# Step 4: Evaluate deviance at this effective mean parameter vector
# Set Z vector to 1 because you've already zeroed out coefficients
D_hat <- -2 * flogis.log(beta_zlv_mean, rep(1, length(beta_zlv_mean)), X_train, y_train)

# Step 5: Effective number of parameters
p_D <- D_bar - D_hat

# Step 6: Compute DIC
DIC <- D_bar + p_D

# Step 7: Print result
cat("Posterior mean deviance (D̄):", round(D_bar, 2), "\n")
cat("Deviance at mean (D̂):", round(D_hat, 2), "\n")
cat("Effective number of parameters (pD):", round(p_D, 2), "\n")
cat("Deviance Information Criterion (DIC):", round(DIC, 2), "\n")

```








